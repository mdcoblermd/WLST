{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","##due to potential module dependencies, we will install DeepTables later\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","!pip install shap\n","import shap\n","import sys\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import seaborn as sn\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc, precision_recall_curve, recall_score, confusion_matrix, brier_score_loss\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import resample\n","sn.set(style='whitegrid')\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)\n","print(\"shap version:\", shap.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["##import your dataset\n","\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["# Import data and specify missing values\n","data = pd.read_csv(file_path, na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'])\n","\n","# Filter out rows where 'TRAUMATYPE' is 26, 'Other/unspecified', or 'Burn'\n","try:\n","  exclude_values = ['26', 'Other/unspecified', 'Burn']\n","  data = data[~data['TRAUMATYPE'].isin(exclude_values)]\n","except:\n","  pass"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##check dataframe to ensure it appears as it should\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing data\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a dataframe of all complications/things not available on admission.  We can remove all of these from the X data set and pick one to be\n","#our Y dataset\n","\n","complications_df=pd.DataFrame()\n","complications_list= [\n","                    'HC_CLABSI', 'HC_DEEPSSI', 'HC_DVTHROMBOSIS', 'HC_ALCOHOLWITHDRAWAL', 'HC_CARDARREST', 'HC_CAUTI',\n","                    'HC_EMBOLISM', 'HC_EXTREMITYCS', 'HC_INTUBATION', 'HC_KIDNEY', 'HC_MI', 'HC_ORGANSPACESSI',\n","                    'HC_OSTEOMYELITIS', 'HC_RESPIRATORY', 'HC_RETURNOR', 'HC_SEPSIS', 'HC_STROKECVA', 'HC_SUPERFICIALINCISIONSSI',\n","                    'HC_PRESSUREULCER', 'HC_UNPLANNEDICU', 'HC_VAPNEUMONIA',\n","                    ##'EDDISCHARGEDISPOSITION',\n","                    'HOSPDISCHARGEDISPOSITION',\n","                    ##'EDDISCHARGEHRS',\n","                    'WITHDRAWALLST',\n","                    'VTEPROPHYLAXISTYPE',\n","                    'TOTALICULOS',\n","                    'TOTALVENTDAYS',\n","                    'VTEPROPHYLAXISHRS',\n","                    'VTEPROPHYLAXISDAYS', 'MORTALITY', 'EDDISCHARGEDAYS','FINALDISCHARGEDAYS','FINALDISCHARGEHRS', 'HMRRHGCTRLSURGDAYS',  'WITHDRAWALLSTHRS',\n","                    ##'AMERICANINDIAN', 'ASIAN', 'BLACK', 'PACIFICISLANDER', 'RACEOTHER', 'WHITE', 'RACE_NA', 'RACE_UK',\n","                    'ISS_05'\n","                    , 'AIS_FACE', 'AIS_NECK', 'AIS_HEAD', 'AIS_THORAX', 'AIS_ABDOMEN', 'AIS_SPINE', 'AIS_UPPEREX', 'AIS_LOWEREX', 'AIS_SKIN', 'AIS_OTHER'\n","                    ##, 'VTEPPXStartOver48', 'VTEPPXStartOver24', 'ICUOver48', 'ICUOver24', 'VentOver48', 'VentOver24'\n","                    , 'VTEPPXStartOver72', 'VTEPPXStartOver96', 'ICUOver72', 'ICUOver96', 'VentOver72', 'VentOver96'\n","                    , 'FacilityTotalWLST', 'factilityTotalPatients', 'FacilityWLSTRate'\n","                    , 'facilityWLSTNew', 'WLSTRateNew', 'WLSTRateNewCensored'\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, in this case, WLST, and move it to a separate dataframe\n","Y_data = pd.DataFrame()\n","Y_data['WLST'] = data['WITHDRAWALLST']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","Y_data['WLST'] = Y_data['WLST'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##remove all unwanted variables as defined above from the input space\n","X_data = data.drop(columns=complications_list)\n","X_data.shape"]},{"cell_type":"code","source":["##need to remove any cases with missing data for our outcome variable\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"],"metadata":{"id":"eh992q4ujyp7"},"id":"eh992q4ujyp7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##here we find which rows in Y have missing values\n","bad_row_index_list=[]\n","for index, row in Y_data.iterrows():\n","    n_missings=row.isnull().sum()\n","    if n_missings>0:\n","        bad_row_index_list.append(index)\n","bad_row_index_list"],"metadata":{"id":"73arQ_SPj1Iy"},"id":"73arQ_SPj1Iy","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b9270983","metadata":{"id":"b9270983"},"outputs":[],"source":["##now remove the bad rows in Y\n","Y_clean = Y_data.drop(bad_row_index_list, axis=0)\n","Y_clean"]},{"cell_type":"code","source":["##ensure all cases with missing values for the outcome have been dropped\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"],"metadata":{"id":"M4GwsrAqj_r2"},"id":"M4GwsrAqj_r2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##and remove bad rows in X\n","X_data=X_data.drop(bad_row_index_list, axis=0)"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##order variables with missing data by percentage\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["##display variables withOUT mising data\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","source":["#remove the good columns (no missing values) from data_missing\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"],"metadata":{"id":"tdQHvSyFzgcU"},"id":"tdQHvSyFzgcU","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##perform median imputation for continuous variable and mode imputation for categorical\n","for c in to_be_cleaned_column_names:\n","    v=X_data_new[c]#get values in this column\n","    v_valid=v[~v.isnull()] # get valid values\n","    if X_data_new[c].dtype == np.dtype('O'): # non-numeric values\n","        X_data_new[c]=X_data_new[c].fillna(v.value_counts().index[0]).astype(object) # the most frequent category\n","    else: # numeric\n","        X_data_new[c]=X_data_new[c].fillna(v_valid.median()) #replace nan with median value"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["##confirm no more missing data in input space\n","X_data_new.isnull().sum().sum()"]},{"cell_type":"code","source":["##verify cleaned dataframe appears as intended\n","X_data_new.head()"],"metadata":{"id":"BZUCy-nyzs1L"},"id":"BZUCy-nyzs1L","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"iWR41vor-_Pu","metadata":{"id":"iWR41vor-_Pu"},"outputs":[],"source":["# Rename the 'TRAUMATYPE' column to 'Penetrating' and map the values to 0 and 1\n","X_data_new['Penetrating'] = X_data_new['TRAUMATYPE'].map({'Penetrating': 1, 'Blunt': 0})\n","\n","# Drop the old 'TRAUMATYPE' column\n","X_data_new.drop(columns=['TRAUMATYPE'], inplace=True)"]},{"cell_type":"code","source":["##remove any additional variables necessary\n","## Remove the \"RACE\" and \"TRANSPORTMODE\" columns, as these are composite varibles that have already been 1 hot encoded\n","columns_to_remove = ['RACE', 'TRANSPORTMODE']\n","X_data_new = X_data_new.drop(columns=columns_to_remove, errors='ignore')"],"metadata":{"id":"yd5lzj7Dzwd3"},"id":"yd5lzj7Dzwd3","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2727c717","metadata":{"id":"2727c717"},"outputs":[],"source":["##first we will convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","##want code to be reusable between different populations of input data.  Not every population will have all of these variables\n","##Therefore, will do everything within separate try/except blocks\n","\n","try:\n","    X_data_new= X_data_new.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","try:\n","    X_data_new['ETHNICITY'] = X_data_new['ETHNICITY'].replace({'Hispanic or Latino': 1, 'Not Hispanic or Latino': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSEYE'] = X_data_new['EMSGCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3,\n","                                                               'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSEYE'] = X_data_new['GCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3, 'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSVERBAL'] = X_data_new['EMSGCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                                     'Confused': 4, 'Oriented': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSMOTOR'] = X_data_new['EMSGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['TBIGCSMOTOR'] = X_data_new['TBIGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSVERBAL'] = X_data_new['GCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                               'Confused': 4, 'Orientated': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSMOTOR'] = X_data_new['GCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                           'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['RESPIRATORYASSISTANCE'] = X_data_new['RESPIRATORYASSISTANCE'].replace({'Assisted Respiratory Rate': 1,\n","                                                                                   'Unassisted Respiratory Rate': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['SUPPLEMENTALOXYGEN'] = X_data_new['SUPPLEMENTALOXYGEN'].replace({'Supplemental Oxygen': 1,\n","                                                                             'No Supplemental Oxygen': 0})\n","except:\n","    pass\n","\n","X_data_new.head()\n","\n","##male coded as 0\n","##female coded as 1\n","\n","##not hispanic coded as 0\n","##hispanic coded as 1"]},{"cell_type":"code","execution_count":null,"id":"3f17738d","metadata":{"id":"3f17738d"},"outputs":[],"source":["##need to convert categorical values to numerical values using one-hot encoding\n","categorical_column=[]\n","for c in X_data_new.columns:\n","    if X_data_new[c].dtype == np.dtype('O', 'category'): # non-numeric values\n","        categorical_column.append(c)\n","categorical_column"]},{"cell_type":"code","execution_count":null,"id":"2770257a","metadata":{"id":"2770257a"},"outputs":[],"source":["##check how many variables we need to one-hot encode\n","len(categorical_column)"]},{"cell_type":"code","execution_count":null,"id":"60e3b4bf","metadata":{"id":"60e3b4bf"},"outputs":[],"source":["##verify dataframe shape\n","X_data_new.shape"]},{"cell_type":"code","execution_count":null,"id":"323357b1","metadata":{"id":"323357b1"},"outputs":[],"source":["##one-hot encode variables above\n","X_clean=pd.get_dummies(X_data_new, columns=categorical_column, sparse=False)\n","X_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"c524ec1d","metadata":{"id":"c524ec1d"},"outputs":[],"source":["##verify cleaned true label dataframe shape\n","Y_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"44d987e0","metadata":{"id":"44d987e0"},"outputs":[],"source":["##verify no missing data in the cleaned input space\n","X_clean.isnull().sum().sum()"]},{"cell_type":"code","source":["##drop patient ID's\n","X_clean.drop(['inc_key'], axis=1, inplace=True)"],"metadata":{"id":"Qc1yYzaQk2T8"},"id":"Qc1yYzaQk2T8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"630434e8","metadata":{"id":"630434e8"},"outputs":[],"source":["##replace boolean values in binary variables to numeric values\n","X_clean = X_clean.replace({True: 1, False: 0})"]},{"cell_type":"code","execution_count":null,"id":"anu9YwWcDMUK","metadata":{"id":"anu9YwWcDMUK"},"outputs":[],"source":["##verify dataframe appears as intended\n","X_clean.head()"]},{"cell_type":"code","execution_count":null,"id":"AbyaxTByRK__","metadata":{"id":"AbyaxTByRK__"},"outputs":[],"source":["##split cleaned input space into training and test sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X_clean, Y_clean, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"62nD6XkiRMfn","metadata":{"id":"62nD6XkiRMfn"},"outputs":[],"source":["##Before converting to Numpy arrays, we generate copies of thed data in tensor format to ensure we have access to\n","##tensor format data if needed\n","\n","X_train_tensor=X_train.copy()\n","Y_train_tensor=Y_train.copy()\n","X_test_tensor=X_test.copy()\n","Y_test_tesnor=Y_test.copy()"]},{"cell_type":"code","execution_count":null,"id":"MkSkefq4RN9D","metadata":{"id":"MkSkefq4RN9D"},"outputs":[],"source":["##convert sets to Numpy arrays:\n","X_train=X_train.values\n","Y_train=Y_train.values.reshape(-1)\n","X_test=X_test.values\n","Y_test=Y_test.values.reshape(-1)"]},{"cell_type":"code","execution_count":null,"id":"PYoFPWDoRPQW","metadata":{"id":"PYoFPWDoRPQW"},"outputs":[],"source":["##now we have X_train, Y_train, X_test, Y_test as numpy arrays\n","\n","scaler=StandardScaler()\n","#get the parameters of the transform\n","scaler.fit(X_train)\n","#normalize the features in the training set\n","X_train_s = scaler.transform(X_train)\n","#normalize the features in the test set\n","X_test_s = scaler.transform(X_test)\n","\n","##lets also scale the tensor copies we created\n","X_train_tensor_s = scaler.transform(X_train_tensor)\n","X_test_tensor_s = scaler.transform(X_test_tensor)"]},{"cell_type":"code","execution_count":null,"id":"E1DWiDLua903","metadata":{"id":"E1DWiDLua903"},"outputs":[],"source":["##further split the training set into a training and validation/calibration set\n","X_train_s_cal, X_val_s_cal, Y_train_cal, Y_val_cal = train_test_split(X_train_s, Y_train, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"u3xRL2Wzbi6k","metadata":{"id":"u3xRL2Wzbi6k"},"outputs":[],"source":["##create a dictionary of model hyper-parameter(s)\n","\n","##for KNN\n","n_list=np.arange(1, 810, 2)\n","param_grid_knc = {'n_neighbors':n_list}\n","\n","##for RF\n","param_grid_rf = {\n","    'n_estimators': [100, 200, 400],        ## Number of trees in the forest\n","    'max_depth': [None, 10, 20, 30],       ## Maximum depth of the trees\n","    'min_samples_split': [2, 5, 10],       ## Minimum number of samples required to split an internal node\n","    'min_samples_leaf': [1, 2, 4],         ## Minimum number of samples required to be at a leaf node\n","    'max_features': ['sqrt']               ## Number of features to consider for the best split\n","}\n","\n","##for LR\n","param_grid_lr = {\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],   ## Inverse of regularization strength\n","    'penalty': ['l1', 'l2'],               ## Regularization type\n","    'solver': ['liblinear', 'saga'],       ## Optimization algorithm\n","    'max_iter': [100, 200, 300]            ## Maximum number of iterations\n","    }\n","\n","##this is for XGBoost\n","param_grid_gb = {\n","    'learning_rate': [0.01, 0.05, 0.1],    ## Learning rate\n","    'max_depth': [3, 5, 7],                ## Maximum depth of the trees\n","    'subsample': [0.6, 0.8, 1.0],          ## Subsample ratio of the training instances\n","    'colsample_bytree': [0.6, 0.8, 1.0],   ## Subsample ratio of columns when constructing each tree.\n","    'n_estimators': [50, 100, 150]        ## Number of trees\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"vtemq7fdbjoE","metadata":{"id":"vtemq7fdbjoE"},"outputs":[],"source":["##now, optimize GB hyperparameters\n","model_gb=xgb.XGBClassifier(random_state=0) #create an empty model\n","##initialize gridsearch\n","gs_gb = GridSearchCV(estimator=model_gb,\n","                  param_grid=param_grid_gb,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","gs_gb.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"l3YpoecxA3qv","metadata":{"id":"l3YpoecxA3qv"},"outputs":[],"source":["##display best parameters\n","gs_gb.best_params_"]},{"cell_type":"code","execution_count":null,"id":"Pga633u7dr9m","metadata":{"id":"Pga633u7dr9m"},"outputs":[],"source":["##now, optimize LR hyperparameters\n","model_lr=LogisticRegression() #create an empty model\n","##initialize gridsearch\n","gs_lr = GridSearchCV(estimator=model_lr,\n","                  param_grid=param_grid_lr,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","#actually perform hyperparmeter optimization\n","gs_lr.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"LjrpHrMYdvj9","metadata":{"id":"LjrpHrMYdvj9"},"outputs":[],"source":["##display best parameters\n","gs_lr.best_params_"]},{"cell_type":"code","execution_count":null,"id":"jG3HRdYEyfk8","metadata":{"id":"jG3HRdYEyfk8"},"outputs":[],"source":["# ##now, optimize RF hyperparameters\n","model_rf=RandomForestClassifier(random_state=0) #create an empty model\n","##initialize gridsearch\n","gs_rf = GridSearchCV(estimator=model_rf,\n","                  param_grid=param_grid_rf,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","#actually perform hyperparmeter optimization\n","gs_rf.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"nGNhumsUzC3F","metadata":{"id":"nGNhumsUzC3F"},"outputs":[],"source":["##display best parameters\n","gs_rf.best_params_"]},{"cell_type":"code","source":["##KNN with GS_CV to optimize hyperparameter\n","from sklearn.neighbors import KNeighborsClassifier\n","model_knno=KNeighborsClassifier() #create an empty model\n","##initialize gridsearch\n","gs_knno = GridSearchCV(estimator=model_knno,\n","                  param_grid=param_grid_knc,\n","                  scoring='roc_auc',\n","                  cv=5, verbose=2)\n","#set cv=5, then it will do 5-fold cross-validation\n","\n","#actually perform hyperparmeter optimization\n","gs_knno.fit(X_train_s_cal, Y_train_cal)"],"metadata":{"id":"RpYOnb_-QT7Q"},"id":"RpYOnb_-QT7Q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##display best parameters\n","gs_knno.best_params_"],"metadata":{"id":"0Xi4n8ENQbwK"},"id":"0Xi4n8ENQbwK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##copy existing dataframes to use in neural networks\n","\n","X_clean_nn_test=X_test_s.copy()\n","Y_clean_nn_test=Y_test.copy()\n","\n","X_clean_nn_train=X_train_s_cal.copy()\n","Y_clean_nn_train=Y_train_cal.copy()\n","\n","X_clean_nn_cal=X_val_s_cal.copy()\n","Y_clean_nn_cal=Y_val_cal.copy()"],"metadata":{"id":"hRdxn8bWcaEs"},"id":"hRdxn8bWcaEs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##ensure data is in pandas dataframe\n","X_train_df = pd.DataFrame(X_clean_nn_train)\n","Y_train_s = pd.Series(Y_clean_nn_train)\n","\n","X_val_df = pd.DataFrame(X_clean_nn_cal)\n","Y_val_s = pd.Series(Y_clean_nn_cal)\n","\n","X_test_df = pd.DataFrame(X_clean_nn_test)\n","Y_test_s = pd.Series(Y_clean_nn_test)"],"metadata":{"id":"Ij2Ga5aD0u9L"},"id":"Ij2Ga5aD0u9L","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install deeptables\n","##revert to sklearn 1.5 to resolve dependency issues\n","!pip install scikit-learn==1.5\n","import deeptables\n","print(\"dt version:\", deeptables.__version__)\n","from deeptables.models.deeptable import DeepTable, ModelConfig\n","from deeptables.models.deepnets import DeepFM, WideDeep, DCN"],"metadata":{"id":"gbBX05h70wUc"},"id":"gbBX05h70wUc","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialize neural network model and fit--try DeepFM first\n","# `auto_discrete` is used to decide wether to discretize continous varibles automatically.\n","conf = ModelConfig(\n","    nets=DeepFM,\n","    metrics=['AUC', 'accuracy'],\n","    auto_discrete=True\n",")\n","dt = DeepTable(config=conf)\n","model, history = dt.fit( X_train_df, Y_train_s, epochs=100, validation_data=(X_val_df, Y_val_s))\n","score = dt.evaluate(X_test_df, Y_test_s)\n","preds = dt.predict(X_test_df)"],"metadata":{"id":"w00K6c-1cd7n"},"id":"w00K6c-1cd7n","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate ROC curve\n","y_pred_prob_ANN = dt.predict_proba(X_clean_nn_test)[:, 1]\n","fpr_ANN, tpr_ANN, thresholds = roc_curve(Y_clean_nn_test, y_pred_prob_ANN)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_ANN = auc(fpr_ANN, tpr_ANN)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_ANN, tpr_ANN, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_ANN:.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"fhMyfjXycfUL"},"id":"fhMyfjXycfUL","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialize neural network model and fit--try WideDeep second\n","# `auto_discrete` is used to decide wether to discretize continous varibles automatically.\n","conf = ModelConfig(\n","    nets=WideDeep,\n","    metrics=['AUC', 'accuracy'],\n","    auto_discrete=True\n",")\n","dt = DeepTable(config=conf)\n","model, history = dt.fit( X_train_df, Y_train_s, epochs=100, validation_data=(X_val_df, Y_val_s))\n","score = dt.evaluate(X_test_df, Y_test_s)\n","preds = dt.predict(X_test_df)"],"metadata":{"id":"Bv_tgKip01tm"},"id":"Bv_tgKip01tm","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate ROC curve\n","y_pred_prob_ANN = dt.predict_proba(X_clean_nn_test)[:, 1]\n","fpr_ANN, tpr_ANN, thresholds = roc_curve(Y_clean_nn_test, y_pred_prob_ANN)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_ANN = auc(fpr_ANN, tpr_ANN)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_ANN, tpr_ANN, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_ANN:.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"ZdfjaO1l03P4"},"id":"ZdfjaO1l03P4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialize neural network model and fit--try DCN last\n","# `auto_discrete` is used to decide wether to discretize continous varibles automatically.\n","conf = ModelConfig(\n","    nets=DCN,\n","    metrics=['AUC', 'accuracy'],\n","    auto_discrete=True\n",")\n","dt = DeepTable(config=conf)\n","model, history = dt.fit( X_train_df, Y_train_s, epochs=100, validation_data=(X_val_df, Y_val_s))\n","score = dt.evaluate(X_test_df, Y_test_s)\n","preds = dt.predict(X_test_df)"],"metadata":{"id":"QhiLxC7L04f5"},"id":"QhiLxC7L04f5","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate ROC curve\n","y_pred_prob_ANN = dt.predict_proba(X_clean_nn_test)[:, 1]\n","fpr_ANN, tpr_ANN, thresholds = roc_curve(Y_clean_nn_test, y_pred_prob_ANN)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_ANN = auc(fpr_ANN, tpr_ANN)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_ANN, tpr_ANN, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_ANN:.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"jbUXQa0X05xs"},"id":"jbUXQa0X05xs","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[{"file_id":"1XL3SGHIBPO06uNNL8pc4jbgyODiCHYmT","timestamp":1734454404403},{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}

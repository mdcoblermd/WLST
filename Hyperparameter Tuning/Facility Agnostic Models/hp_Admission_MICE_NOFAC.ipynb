{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","##due to potential module dependencies, we will install DeepTables later\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","!pip install shap\n","import time\n","import os\n","import shap\n","import sys\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import seaborn as sn\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, ParameterGrid\n","from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc, precision_recall_curve, recall_score, confusion_matrix, brier_score_loss\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import resample\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer\n","sn.set(style='whitegrid')\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)\n","print(\"shap version:\", shap.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["##import your dataset\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["# Import data and specify missing values\n","data = pd.read_csv(file_path, na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'])\n","\n","# Filter out rows where 'TRAUMATYPE' is 26, 'Other/unspecified', or 'Burn'\n","try:\n","  exclude_values = ['26', 'Other/unspecified', 'Burn']\n","  data = data[~data['TRAUMATYPE'].isin(exclude_values)]\n","except:\n","  pass"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##check dataframe to ensure it appears as it should\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing data\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a dataframe of all complications/things not available on admission.  We can remove all of these from the X data set and pick one to be\n","#our Y dataset\n","\n","complications_df=pd.DataFrame()\n","complications_list= [\n","                    'HC_CLABSI', 'HC_DEEPSSI', 'HC_DVTHROMBOSIS', 'HC_ALCOHOLWITHDRAWAL', 'HC_CARDARREST', 'HC_CAUTI',\n","                    'HC_EMBOLISM', 'HC_EXTREMITYCS', 'HC_INTUBATION', 'HC_KIDNEY', 'HC_MI', 'HC_ORGANSPACESSI',\n","                    'HC_OSTEOMYELITIS', 'HC_RESPIRATORY', 'HC_RETURNOR', 'HC_SEPSIS', 'HC_STROKECVA', 'HC_SUPERFICIALINCISIONSSI',\n","                    'HC_PRESSUREULCER', 'HC_UNPLANNEDICU', 'HC_VAPNEUMONIA',\n","                    ##'EDDISCHARGEDISPOSITION',\n","                    'HOSPDISCHARGEDISPOSITION',\n","                    ##'EDDISCHARGEHRS',\n","                    'WITHDRAWALLST',\n","                    'VTEPROPHYLAXISTYPE',\n","                    'TOTALICULOS',\n","                    'TOTALVENTDAYS',\n","                    'VTEPROPHYLAXISHRS',\n","                    'VTEPROPHYLAXISDAYS', 'MORTALITY', 'EDDISCHARGEDAYS','FINALDISCHARGEDAYS','FINALDISCHARGEHRS', 'HMRRHGCTRLSURGDAYS',  'WITHDRAWALLSTHRS',\n","                    'AMERICANINDIAN', 'ASIAN', 'BLACK', 'PACIFICISLANDER', 'RACEOTHER', 'WHITE', 'RACE_NA', 'RACE_UK',\n","                    'TM_GROUNDAMBULANCE', 'TM_HELICOPTERAMBULANCE', 'TM_FIXEDWINGAMBULANCE', 'TM_PRIVPUBVEHWALKIN', 'TM_POLICE', 'TM_OTHER', 'TM_NA', 'TM_UK',\n","                    'ISS_05'\n","                    , 'AIS_FACE', 'AIS_NECK', 'AIS_HEAD', 'AIS_THORAX', 'AIS_ABDOMEN', 'AIS_SPINE', 'AIS_UPPEREX', 'AIS_LOWEREX', 'AIS_SKIN', 'AIS_OTHER'\n","                    , 'VTEPPXStartOver48', 'VTEPPXStartOver24', 'ICUOver48', 'ICUOver24', 'VentOver48', 'VentOver24'\n","                    , 'VTEPPXStartOver72', 'VTEPPXStartOver96', 'ICUOver72', 'ICUOver96', 'VentOver72', 'VentOver96'\n","                    , 'FacilityTotalWLST', 'factilityTotalPatients', 'FacilityWLSTRate', 'FacilityKey'\n","                    , 'facilityWLSTNew', 'WLSTRateNew', 'WLSTRateNewCensored'\n","                    , \"facilityPatientsNew\", \"WLSTRateCensorNormal\", \"HOSPITALTYPE\", \"STATEDESIGNATION\", \"TEACHINGSTATUS\", \"VERIFICATIONLEVEL\"\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, in this case, WLST, and move it to a separate dataframe\n","Y_data = pd.DataFrame()\n","Y_data['WLST'] = data['WITHDRAWALLST']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","\n","Y_data['WLST'] = Y_data['WLST'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##remove all unwanted variables as defined above from the input space\n","X_data = data.drop(columns=complications_list)\n","X_data.shape"]},{"cell_type":"code","execution_count":null,"id":"eh992q4ujyp7","metadata":{"id":"eh992q4ujyp7"},"outputs":[],"source":["##need to remove any cases with missing data for our outcome variable\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"]},{"cell_type":"code","execution_count":null,"id":"73arQ_SPj1Iy","metadata":{"id":"73arQ_SPj1Iy"},"outputs":[],"source":["##here we find which rows in Y have missing values\n","\n","bad_row_index_list=[]\n","for n in range(0, Y_data.shape[0]):\n","    n_missings=Y_data.iloc[n,:].isnull().sum()\n","    if n_missings>0:\n","        bad_row_index_list.append(n)\n","bad_row_index_list"]},{"cell_type":"code","execution_count":null,"id":"b9270983","metadata":{"id":"b9270983"},"outputs":[],"source":["##now remove the bad rows in Y\n","Y_clean = Y_data.drop(bad_row_index_list, axis=0)\n","Y_clean"]},{"cell_type":"code","execution_count":null,"id":"ZWG9Tdo_gk5e","metadata":{"id":"ZWG9Tdo_gk5e"},"outputs":[],"source":["##ensure all cases with missing values for the outcome have been dropped\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"]},{"cell_type":"code","execution_count":null,"id":"M4GwsrAqj_r2","metadata":{"id":"M4GwsrAqj_r2"},"outputs":[],"source":["##and remove bad rows in X\n","X_data=X_data.drop(bad_row_index_list, axis=0)"]},{"cell_type":"code","execution_count":null,"id":"Vh_ZoLAhQrng","metadata":{"id":"Vh_ZoLAhQrng"},"outputs":[],"source":["# Rename the 'TRAUMATYPE' column to 'Penetrating' and map the values to 0 and 1\n","X_data['Penetrating'] = X_data['TRAUMATYPE'].map({'Penetrating': 1, 'Blunt': 0})\n","\n","# Drop the old 'TRAUMATYPE' column\n","X_data.drop(columns=['TRAUMATYPE'], inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"zmsOgiisQ4RV","metadata":{"id":"zmsOgiisQ4RV"},"outputs":[],"source":["##drop patient record number as its not useful in making predictions\n","\n","columns_to_remove = ['inc_key']\n","X_data = X_data.drop(columns=columns_to_remove, errors='ignore')"]},{"cell_type":"code","execution_count":null,"id":"5FqhaevYRJBK","metadata":{"id":"5FqhaevYRJBK"},"outputs":[],"source":["##first we will convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","##want code to be reusable between different populations of input data.  Not every population will have all of these variables\n","##Therefore, will do everything within separate try/except blocks\n","\n","try:\n","    X_data= X_data.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","try:\n","    X_data['ETHNICITY'] = X_data['ETHNICITY'].replace({'Hispanic or Latino': 1, 'Not Hispanic or Latino': 0})\n","except:\n","    pass\n","try:\n","    X_data['EMSGCSEYE'] = X_data['EMSGCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3,\n","                                                               'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data['GCSEYE'] = X_data['GCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3, 'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data['EMSGCSVERBAL'] = X_data['EMSGCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                                     'Confused': 4, 'Oriented': 5})\n","except:\n","    pass\n","try:\n","    X_data['EMSGCSMOTOR'] = X_data['EMSGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data['TBIGCSMOTOR'] = X_data['TBIGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data['GCSVERBAL'] = X_data['GCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                               'Confused': 4, 'Orientated': 5})\n","except:\n","    pass\n","try:\n","    X_data['GCSMOTOR'] = X_data['GCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                           'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data['RESPIRATORYASSISTANCE'] = X_data['RESPIRATORYASSISTANCE'].replace({'Assisted Respiratory Rate': 1,\n","                                                                                   'Unassisted Respiratory Rate': 0})\n","except:\n","    pass\n","try:\n","    X_data['SUPPLEMENTALOXYGEN'] = X_data['SUPPLEMENTALOXYGEN'].replace({'Supplemental Oxygen': 1,\n","                                                                             'No Supplemental Oxygen': 0})\n","except:\n","    pass\n","\n","X_data.head()\n","\n","##male coded as 0\n","##female coded as 1\n","\n","##not hispanic coded as 0\n","##hispanic coded as 1"]},{"cell_type":"code","execution_count":null,"id":"X8qJPvjMR1Ca","metadata":{"id":"X8qJPvjMR1Ca"},"outputs":[],"source":["##replace boolean values in binary variables to numeric values\n","X_data = X_data.replace({True: 1, False: 0})"]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##order variables with missing data by percentage\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##display variables withOUT mising data\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["#remove the good columns (no missing values) from data_missing\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","pd.set_option('display.max_rows', None)\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##actually drop bad variables\n","X_data_new=X_data.drop(columns=bad_column_names, axis=1)\n","\n","##check for which variables still have missing data (<50% missing values)\n","Missing = X_data_new.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["#check for columns with less than 50% missing that need to be cleaned\n","pd.set_option('display.max_rows', None)\n","to_be_cleaned_column_names = data_missing[data_missing <50].index\n","for col in X_data_new:\n","    print(col)"]},{"cell_type":"code","execution_count":null,"id":"8jhnKpBUJW9C","metadata":{"id":"8jhnKpBUJW9C"},"outputs":[],"source":["continuous_vars = [\n","    \"AGEYEARS\", \"EMSPULSERATE\", \"EMSRESPIRATORYRATE\", \"EMSTOTALGCS\", \"EMSDISPATCHDAYS\",\n","    \"EMSSCENEHRS\", \"EMSSCENEDAYS\", \"EMSHRS\", \"EMSDAYS\", \"SBP\", \"PULSERATE\", \"TEMPERATURE\",\n","    \"RESPIRATORYRATE\", \"PULSEOXIMETRY\", \"HEIGHT\", \"WEIGHT\", \"TOTALGCS\", \"ALCOHOLSCREENRESULT\",\n","    \"EDDISCHARGEHRS\", \"TBIHIGHESTTOTALGCS\", \"TBIGCSMOTOR\", \"BLOODUNITS\", \"PLASMAUNITS\",\n","    \"NumberOfInjuries\", \"mFI\"\n","]\n","\n","categorical_vars = [\n","    \"SEX\", \"RACE\", \"ETHNICITY\", \"MECHANISM\", \"INTENT\", \"WORKRELATED\", \"ABUSEREPORT\",\n","    \"PROTDEV_NONE\", \"PROTDEV_LAP_BELT\", \"PROTDEV_PER_FLOAT\", \"PROTDEV_PROTECT_GEAR\",\n","    \"PROTDEV_EYE_PROTECT\", \"PROTDEV_CHILD_RESTRAINT\", \"PROTDEV_HELMET\", \"PROTDEV_AIRBAG_PRESENT\",\n","    \"PROTDEV_PROTECT_CLOTH\", \"PROTDEV_SHOULDER_BELT\", \"PROTDEV_OTHER\", \"PROTDEV_NA\", \"PROTDEV_UK\",\n","    \"AIRBAG_NOTDEPLOYED\", \"AIRBAG_DEPLOYED_FRNT\", \"AIRBAG_DEPLOYED_SIDE\", \"AIRBAG_DEPLOYED_OTHER\",\n","    \"AIRBAG_DEPLOYED_NA\", \"AIRBAG_DEPLOYED_UK\", \"TRANSPORTMODE\", \"INTERFACILITYTRANSFER\",\n","    \"PREHOSPITALCARDIACARREST\", \"TCCGCSLE13\", \"TCC10RR29\", \"TCCPEN\", \"TCCCHEST\", \"TCCLONGBONE\",\n","    \"TCCCRUSHED\", \"TCCAMPUTATION\", \"TCCPELVIC\", \"TCCSKULLFRACTURE\", \"TCCPARALYSIS\", \"TCC_NA\",\n","    \"TCC_UK\", \"VPOFALLADULT\", \"VPOFALLCHILD\", \"VPOCRASHINTRUSION\", \"VPOCRASHEJECT\",\n","    \"VPOCRASHDEATH\", \"VPOCRASHTELEMETRY\", \"VPOAUTOPEDIMPACT\", \"VPOMOTORCYCLECRASH\",\n","    \"VPO65SBP110\", \"VPOANTICOAGULANT\", \"VPOPREGNANCY20WKS\", \"VPOEMSJUDGE\", \"VPOBURNS\",\n","    \"VPOTRAUMABURNS\", \"VPO_NA\", \"VPO_UK\", \"RESPIRATORYASSISTANCE\", \"SUPPLEMENTALOXYGEN\",\n","    \"GCSQ_SEDATEDPARALYZED\", \"GCSQ_EYEOBSTRUCTION\", \"GCSQ_INTUBATED\", \"GCSQ_VALID\", \"GCSQ_NA\",\n","    \"GCSQ_UK\", \"DRGSCR_AMPHETAMINE\", \"DRGSCR_BARBITURATE\", \"DRGSCR_BENZODIAZEPINES\",\n","    \"DRGSCR_COCAINE\", \"DRGSCR_METHAMPHETAMINE\", \"DRGSCR_ECSTASY\", \"DRGSCR_METHADONE\",\n","    \"DRGSCR_OPIOID\", \"DRGSCR_OXYCODONE\", \"DRGSCR_PHENCYCLIDINE\", \"DRGSCR_TRICYCLICDEPRESS\",\n","    \"DRGSCR_CANNABINOID\", \"DRGSCR_OTHER\", \"DRGSCR_NONE\", \"DRGSCR_NOTTESTED\", \"DRGSCR_UK\",\n","    \"DRGSCR_NA\", \"ALCOHOLSCREEN\", \"EDDISCHARGEDISPOSITION\", \"DEATHINED\", \"TBIPUPILLARYRESPONSE\",\n","    \"TBIMIDLINESHIFT\", \"PMGCSQ_SEDATEDPARALYZED\", \"PMGCSQ_EYEOBSTRUCTION\", \"PMGCSQ_INTUBATED\",\n","    \"PMGCSQ_VALID\", \"PMGCSQ_NA\", \"PMGCSQ_UK\", \"ICPEVDRAIN\", \"ICPPARENCH\", \"ICPO2MONITOR\",\n","    \"ICPJVBULB\", \"ICPNONE\", \"ICP_NA\", \"ICP_UK\", \"BLOODBINARY\", \"PLASMABINARY\", \"PLATELETSBINARY\",\n","    \"CRYOBINARY\", \"ESLIVER\", \"ESSPLEEN\", \"ESKIDNEY\", \"ESPELVIS\", \"ESRETROPERI\",\n","    \"ESVASCULAR\", \"ESAORTA\", \"ESOTHER\", \"ES_UK\", \"ES_NA\", \"PRIMARYMETHODPAYMENT\",\n","    \"CC_ADHD\", \"CC_ADLC\", \"CC_ALCOHOLISM\", \"CC_ANGINAPECTORIS\",\n","    \"CC_ANTICOAGULANT\", \"CC_BLEEDING\", \"CC_CHEMO\", \"CC_CIRRHOSIS\", \"CC_CONGENITAL\", \"CC_COPD\",\n","    \"CC_CVA\", \"CC_DEMENTIA\", \"CC_DIABETES\", \"CC_DISCANCER\", \"CC_FUNCTIONAL\", \"CC_CHF\",\n","    \"CC_HYPERTENSION\", \"CC_MI\", \"CC_PAD\", \"CC_PREMATURITY\", \"CC_MENTALPERSONALITY\", \"CC_RENAL\",\n","    \"CC_SMOKING\", \"CC_STEROID\", \"CC_SUBSTANCEABUSE\", \"IntracranialVascularInjury\", \"BrainStemInjury\",\n","    \"EDH\", \"SAH\", \"SDH\", \"SkullFx\", \"DAI\", \"NeckVascularInjury\", \"ThoracicVascularInjury\",\n","    \"AeroDigestiveInjury\", \"CardiacInjury\", \"LungInjury\", \"AbdominalVascular\", \"RibFx\",\n","    \"KidneyInjury\", \"StomachInjury\", \"SpleenInjury\", \"UroGenInternalInjury\", \"SCI\", \"SpineFx\",\n","    \"UEAmputation\", \"UEVascularInjury\", \"UELongBoneFx\", \"LEVascularInjury\", \"PelvicFx\",\n","    \"LEAmputation\", \"PancreasInjury\", \"LELongBoneFx\", \"LiverInjury\", \"ColorectalInjury\",\n","    \"SmallBowelInjury\", \"isolatedTBI\", \"missingGCS\", \"missingAge\", \"missingSex\", \"missingType\",\n","    \"missingSBP\", \"missingHR\", \"missingRR\", \"missingPulseOx\", \"missingHeight\", \"missingWeight\",\n","    \"missingEDDispo\", \"missingRBC\", \"missingPlasma\", \"Penetrating\"\n","]\n","\n","# Continuous variables that must be integers\n","int_constrained_vars = [\n","    \"AGEYEARS\", \"EMSPULSERATE\", \"EMSRESPIRATORYRATE\", \"EMSTOTALGCS\", \"EMSDISPATCHDAYS\",\n","    \"EMSSCENEDAYS\", \"EMSDAYS\", \"SBP\", \"PULSERATE\", \"RESPIRATORYRATE\", \"PULSEOXIMETRY\",\n","    \"TOTALGCS\", \"TBIHIGHESTTOTALGCS\", \"TBIGCSMOTOR\", \"BLOODUNITS\", \"PLASMAUNITS\",\n","    \"NumberOfInjuries\", \"mFI\"\n","]\n","\n","time_pairs = [\n","    (\"EMSSCENEDAYS\", \"EMSSCENEHRS\"),\n","    (\"EMSDAYS\", \"EMSHRS\"),\n","]"]},{"cell_type":"code","execution_count":null,"id":"lr5iE62SJ6Qr","metadata":{"id":"lr5iE62SJ6Qr"},"outputs":[],"source":["print(X_data_new.shape)\n","print(len(continuous_vars))\n","print(len(categorical_vars))"]},{"cell_type":"code","execution_count":null,"id":"1jyZsMwoXq50","metadata":{"id":"1jyZsMwoXq50"},"outputs":[],"source":["##train/test split time\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 1) Start with your full feature matrix X_data_new and your outcome Y_clean.\n","# ───────────────────────────────────────────────────────────────────────────────\n","\n","X_full = X_data_new.copy()\n","# If Y_clean is a one-column DataFrame, extract it as a Series. Otherwise adjust accordingly.\n","Y_full = Y_clean.iloc[:, 0]\n","\n","# 2) Identify which columns are categorical / continuous / integer‐constrained.\n","categorical_mice = [c for c in categorical_vars if c in X_full.columns]\n","continuous_mice  = [c for c in continuous_vars  if c in X_full.columns]\n","int_mice         = [c for c in int_constrained_vars if c in X_full.columns]\n","\n","# 3) Convert any \"nan\" strings into real np.nan in the categorical columns.\n","X_full[categorical_mice] = (\n","    X_full[categorical_mice]\n","      .replace(\"nan\", np.nan)\n","      .astype(str)\n","      .replace(\"nan\", np.nan)\n",")\n","\n","# 4) Ordinal‐encode all categorical_mice columns at once (so train/test share the same mapping).\n","ordinal_encoder_mice = OrdinalEncoder(\n","    handle_unknown=\"use_encoded_value\",\n","    unknown_value=-1\n",")\n","X_full[categorical_mice] = ordinal_encoder_mice.fit_transform(X_full[categorical_mice])\n","\n","# 5) Now do the first split: full → (train_raw, test_raw).\n","#    Since your outcome is binary, we can stratify on it.\n","X_train_raw, X_test_raw, Y_train, Y_test = train_test_split(\n","    X_full,\n","    Y_full,\n","    test_size=0.20,\n","    random_state=0,\n","    stratify=Y_full\n",")\n","\n","print(\"Completed train/test split:\")\n","print(f\"   X_train_raw: {X_train_raw.shape}, X_test_raw: {X_test_raw.shape}\")\n","print(f\"   Y_train: {Y_train.shape},      Y_test: {Y_test.shape}\")\n","\n","# 6) Next, split X_train_raw + Y_train into (train_for_impute_raw, calibration_raw).\n","#    We'll use calibration_raw later to calibrate the model.\n","#    Use stratification here as well (since Y_train is still binary).\n","X_train_for_impute_raw, X_cal_raw, Y_train_for_impute, Y_cal = train_test_split(\n","    X_train_raw,\n","    Y_train,\n","    test_size=0.20,\n","    random_state=0,\n","    stratify=Y_train\n",")\n","\n","print(\"\\nSplit train_raw into train_for_impute and calibration:\")\n","print(f\"   X_train_for_impute_raw: {X_train_for_impute_raw.shape}, Y_train_for_impute: {Y_train_for_impute.shape}\")\n","print(f\"   X_cal_raw: {X_cal_raw.shape},            Y_cal: {Y_cal.shape}\")\n"]},{"cell_type":"code","execution_count":null,"id":"0Ii8GYL_XrDf","metadata":{"id":"0Ii8GYL_XrDf"},"outputs":[],"source":["##now for MICE\n","\n","# ──────────────────────────────────────────────────────────────────────────────\n","# WRAPPER: RandomForestRegressor that supports predict(X, return_std=True)\n","# ──────────────────────────────────────────────────────────────────────────────\n","class RFWithStd(RandomForestRegressor):\n","    \"\"\"\n","    Subclass of RandomForestRegressor whose `predict(..., return_std=True)`\n","    returns (mean_prediction, std_prediction) across all trees.\n","    \"\"\"\n","    def predict(self, X, return_std=False):\n","        # Gather per-tree predictions\n","        all_tree_preds = np.vstack([tree.predict(X) for tree in self.estimators_])\n","        means = np.mean(all_tree_preds, axis=0)\n","        if not return_std:\n","            return means\n","        stds = np.std(all_tree_preds, axis=0, ddof=1)\n","        return means, stds\n","\n","# ──────────────────────────────────────────────────────────────────────────────\n","# RF‐MICE with sample_posterior=True using RFWithStd\n","# ──────────────────────────────────────────────────────────────────────────────\n","\n","seeds = [100, 200, 300]\n","imputed_X_train_list = []\n","imputed_X_cal_list   = []\n","imputed_X_test_list  = []\n","\n","print(f\"Running RF‐MICE on TRAIN_FOR_IMPUTE only (n_train={X_train_for_impute_raw.shape[0]})\\n\")\n","\n","for seed in seeds:\n","    start_time = time.time()\n","\n","    rf_imputer = IterativeImputer(\n","        estimator=RFWithStd(\n","            n_estimators=100,\n","            max_depth=10,\n","            n_jobs=-1,\n","            random_state=seed\n","        ),\n","        max_iter=3,             # more iterations for convergence\n","        sample_posterior=True,  # draw from posterior predictive distribution\n","        random_state=seed\n","    )\n","\n","    # 1) Fit on TRAIN_FOR_IMPUTE raw\n","    rf_imputer.fit(X_train_for_impute_raw)\n","\n","    # 2) Impute TRAIN_FOR_IMPUTE, CAL, TEST\n","    for (data_raw, out_list) in [\n","        (X_train_for_impute_raw, imputed_X_train_list),\n","        (X_cal_raw,               imputed_X_cal_list),\n","        (X_test_raw,              imputed_X_test_list),\n","    ]:\n","        arr_imp = rf_imputer.transform(data_raw)\n","        df_imp = pd.DataFrame(arr_imp, columns=data_raw.columns, index=data_raw.index)\n","\n","        # 3) Round & clip categorical columns back to integer codes\n","        for idx, col in enumerate(categorical_mice):\n","            n_cat = len(ordinal_encoder_mice.categories_[idx])\n","            df_imp[col] = df_imp[col].round().clip(0, n_cat - 1).astype(int)\n","\n","        # 4) Decode categorical columns back to original labels\n","        df_imp[categorical_mice] = ordinal_encoder_mice.inverse_transform(df_imp[categorical_mice])\n","\n","        # 5) Clip continuous columns to original min/max from X_full\n","        for col in continuous_mice:\n","            col_min = X_full[col].min(skipna=True)\n","            col_max = X_full[col].max(skipna=True)\n","            df_imp[col] = df_imp[col].clip(lower=col_min, upper=col_max)\n","\n","        # 6) Round & cast integer‐constrained columns\n","        for col in int_mice:\n","            if col in df_imp.columns:\n","                df_imp[col] = np.round(df_imp[col]).astype(\"Int64\")\n","\n","        # 7) Domain‐specific consistency checks for GCS & alcohol\n","        if \"TBIHIGHESTTOTALGCS\" in df_imp.columns and \"TBIGCSMOTOR\" in df_imp.columns:\n","            required_min = df_imp[\"TBIGCSMOTOR\"] + 2\n","            df_imp[\"TBIHIGHESTTOTALGCS\"] = np.maximum(df_imp[\"TBIHIGHESTTOTALGCS\"], required_min)\n","        if \"ALCOHOLSCREEN\" in df_imp.columns and \"ALCOHOLSCREENRESULT\" in df_imp.columns:\n","            df_imp.loc[df_imp[\"ALCOHOLSCREEN\"] == 0, \"ALCOHOLSCREENRESULT\"] = 0\n","\n","\n","        # 8) Fill any remaining missing values (mode for categorical, median for continuous)\n","        for col in categorical_mice:\n","            train_mode = df_imp[col].mode(dropna=True)[0]\n","            df_imp[col] = df_imp[col].fillna(train_mode)\n","        for col in continuous_mice:\n","            train_med = df_imp[col].median()\n","            df_imp[col] = df_imp[col].fillna(train_med)\n","\n","        # 9) Cast categorical columns to pandas “category” dtype\n","        for col in categorical_mice:\n","            df_imp[col] = df_imp[col].astype(\"category\")\n","\n","        out_list.append(df_imp)\n","\n","    elapsed = time.time() - start_time\n","    print(f\"  → Seed {seed} done in {elapsed:.1f} seconds.\")\n","\n","print(\"RF‐MICE completed\")\n"]},{"cell_type":"code","execution_count":null,"id":"B6oHkEu_WX9R","metadata":{"id":"B6oHkEu_WX9R"},"outputs":[],"source":["##save our imputed datasets to a google drive directory\n","\n","# 1) save to your drive directory.  Change the name of <MY_SAV_DIR> to this directory.\n","save_dir = \"<MY_SAV_DIR>\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# 2) Define custom filenames for each imputation\n","#    You can pick any descriptive names you like.\n","\n","train_names = [\"nofac_train_seed100_admission\", \"nofac_train_seed200_admission\", \"nofac_train_seed300_admission\"]\n","cal_names   = [\"nofac_cal_seed100_admission\",   \"nofac_cal_seed200_admission\",   \"nofac_cal_seed300_admission\"]\n","test_names  = [\"nofac_test_seed100_admission\",  \"nofac_test_seed200_admission\",  \"nofac_test_seed300_admission\"]\n","\n","\n","# 3) Save each DataFrame under its custom name\n","for i, df in enumerate(imputed_X_train_list):\n","    filename = f\"{train_names[i]}.pkl\"\n","    df.to_pickle(os.path.join(save_dir, filename))\n","\n","for i, df in enumerate(imputed_X_cal_list):\n","    filename = f\"{cal_names[i]}.pkl\"\n","    df.to_pickle(os.path.join(save_dir, filename))\n","\n","for i, df in enumerate(imputed_X_test_list):\n","    filename = f\"{test_names[i]}.pkl\"\n","    df.to_pickle(os.path.join(save_dir, filename))\n","\n","print(f\"Saved imputations to {save_dir} with custom names.\")\n"]},{"cell_type":"code","execution_count":null,"id":"QGVRfvubMfjv","metadata":{"id":"QGVRfvubMfjv"},"outputs":[],"source":["##now for variable summary pre and post imputation\n","\n","# Lists of imputed variables (must already exist in your notebook)\n","# continuous_mice = [ ... ]  # list of continuous columns that were imputed\n","# categorical_mice = [ ... ] # list of categorical columns that were imputed\n","\n","# -----------------------------------------------------------------------------\n","# Helper: summary for continuous variables\n","# -----------------------------------------------------------------------------\n","def summarise_continuous(df, vars_list):\n","    stats = []\n","    for var in vars_list:\n","        s = df[var].dropna()\n","        mean = s.mean()\n","        median = s.median()\n","        std = s.std()\n","        q1 = s.quantile(0.25)\n","        q3 = s.quantile(0.75)\n","        iqr = q3 - q1\n","        stats.append({\n","            'variable': var,\n","            'mean': mean,\n","            'median': median,\n","            'std': std,\n","            'IQR': iqr\n","        })\n","    return pd.DataFrame(stats).set_index('variable')\n","\n","# -----------------------------------------------------------------------------\n","# Helper: summary for categorical variables\n","# -----------------------------------------------------------------------------\n","def summarise_categorical(df, vars_list):\n","    summaries = {}\n","    for var in vars_list:\n","        counts = df[var].value_counts(dropna=True)\n","        props = counts / counts.sum()\n","        mode = df[var].mode(dropna=True)\n","        mode = mode.iloc[0] if not mode.empty else np.nan\n","        mode_prop = props.get(mode, np.nan)\n","        summaries[var] = {\n","            'mode': mode,\n","            'mode_proportion': mode_prop,\n","            'levels_proportions': props.to_dict()\n","        }\n","    return summaries\n","\n","# -----------------------------------------------------------------------------\n","# 1) Pre-imputation summaries on TRAIN_FOR_IMPUTE raw\n","# -----------------------------------------------------------------------------\n","pre_train = X_train_for_impute_raw.copy()\n","\n","cont_pre_summary = summarise_continuous(pre_train, continuous_mice)\n","cat_pre_summary = summarise_categorical(pre_train, categorical_mice)\n","\n","# -----------------------------------------------------------------------------\n","# 2) Post-imputation summaries (each of the 3 imputations on TRAIN_FOR_IMPUTE)\n","# -----------------------------------------------------------------------------\n","cont_post_list = []\n","cat_post_list = []\n","\n","for i, df_imp in enumerate(imputed_X_train_list):\n","    # 2a) Continuous stats for imputation i\n","    cont_stats = summarise_continuous(df_imp, continuous_mice).rename(\n","        columns=lambda c: f\"{c}_imp{i}\"\n","    )\n","    cont_post_list.append(cont_stats)\n","\n","    # 2b) Categorical stats (mode + mode proportion) for imputation i\n","    cat_stats = pd.DataFrame([\n","        {\n","            'variable': var,\n","            'mode_imp': df_imp[var].mode(dropna=True).iloc[0] if not df_imp[var].mode(dropna=True).empty else np.nan,\n","            'mode_prop_imp': df_imp[var].value_counts(normalize=True).get(\n","                df_imp[var].mode(dropna=True).iloc[0], np.nan\n","            )\n","        }\n","        for var in categorical_mice\n","    ]).set_index('variable').rename(columns=lambda c: f\"{c}_{i}\")\n","    cat_post_list.append(cat_stats)\n","\n","# 2c) Combine continuous stats across imputations\n","cont_post_summary = pd.concat(cont_post_list, axis=1)\n","\n","# 2d) Combine categorical stats across imputations\n","cat_post_summary = pd.concat(cat_post_list, axis=1)\n","\n","# -----------------------------------------------------------------------------\n","# 3) Display pre- vs post-imputation summaries for continuous\n","# -----------------------------------------------------------------------------\n","print(\"=== Continuous variables pre-imputation (TRAIN_FOR_IMPUTE) ===\")\n","display(cont_pre_summary)\n","\n","print(\"=== Continuous variables post-imputation (TRAIN_FOR_IMPUTE) ===\")\n","display(cont_post_summary)\n","\n","# -----------------------------------------------------------------------------\n","# 4) Display pre- vs post-imputation summaries for categorical\n","# -----------------------------------------------------------------------------\n","print(\"=== Categorical variables pre-imputation (TRAIN_FOR_IMPUTE) ===\")\n","for var, info in cat_pre_summary.items():\n","    print(\n","        f\"{var}: mode={info['mode']}, \"\n","        f\"mode_proportion={info['mode_proportion']:.3f}, \"\n","        f\"level_proportions={info['levels_proportions']}\"\n","    )\n","\n","print(\"\\n=== Categorical variables post-imputation (TRAIN_FOR_IMPUTE) ===\")\n","display(cat_post_summary)\n"]},{"cell_type":"code","execution_count":null,"id":"Y9MQ3v0BKs34","metadata":{"id":"Y9MQ3v0BKs34"},"outputs":[],"source":["# ──────────────────────────────────────────────────────────────────────────────\n","# Sanity‐check A: For each imputed TRAIN/CAL/TEST, confirm categorical dtypes & cardinalities\n","# ──────────────────────────────────────────────────────────────────────────────\n","\n","for i in range(len(imputed_X_train_list)):\n","    print(f\"\\n--- Imputation {i} (TRAIN_FOR_IMPUTE) ---\")\n","    df_train = imputed_X_train_list[i]\n","\n","    # 1) Cardinality of each categorical_mice column\n","    card_train = df_train[categorical_mice].nunique().sort_values(ascending=False)\n","    print(\"  TRAIN_FOR_IMPUTE cardinalities:\\n\", card_train)\n","\n","    # 2) dtype of each categorical_mice column\n","    dtypes_train = df_train[categorical_mice].dtypes\n","    print(\"  TRAIN_FOR_IMPUTE dtypes:\\n\", dtypes_train)\n","\n","    print(f\"\\n--- Imputation {i} (CALIBRATION) ---\")\n","    df_cal = imputed_X_cal_list[i]\n","\n","    card_cal = df_cal[categorical_mice].nunique().sort_values(ascending=False)\n","    print(\"  CALIBRATION cardinalities:\\n\", card_cal)\n","\n","    dtypes_cal = df_cal[categorical_mice].dtypes\n","    print(\"  CALIBRATION dtypes:\\n\", dtypes_cal)\n","\n","    print(f\"\\n--- Imputation {i} (TEST) ---\")\n","    df_test = imputed_X_test_list[i]\n","\n","    card_test = df_test[categorical_mice].nunique().sort_values(ascending=False)\n","    print(\"  TEST cardinalities:\\n\", card_test)\n","\n","    dtypes_test = df_test[categorical_mice].dtypes\n","    print(\"  TEST dtypes:\\n\", dtypes_test)\n","\n","    print(\"─\" * 60)\n"]},{"cell_type":"code","execution_count":null,"id":"giEhapWAK_6v","metadata":{"id":"giEhapWAK_6v"},"outputs":[],"source":["# ──────────────────────────────────────────────────────────────────────────────\n","# Sanity‐check B: Ensure zero missing values in TRAIN_FOR_IMPUTE / CAL / TEST\n","# ──────────────────────────────────────────────────────────────────────────────\n","\n","for i in range(len(imputed_X_train_list)):\n","    df_train = imputed_X_train_list[i]\n","    n_missing_train = df_train.isnull().sum().sum()\n","\n","    df_cal = imputed_X_cal_list[i]\n","    n_missing_cal = df_cal.isnull().sum().sum()\n","\n","    df_test = imputed_X_test_list[i]\n","    n_missing_test = df_test.isnull().sum().sum()\n","\n","    print(f\"Imputation {i}:\")\n","    print(f\"  TRAIN_FOR_IMPUTE missing count = {n_missing_train}\")\n","    print(f\"  CALIBRATION missing count   = {n_missing_cal}\")\n","    print(f\"  TEST missing count          = {n_missing_test}\")\n","    print(\"─\" * 60)\n"]},{"cell_type":"code","execution_count":null,"id":"wdzUxjb5XrHc","metadata":{"id":"wdzUxjb5XrHc"},"outputs":[],"source":["##now for one-hot encoding\n","\n","encoded_X_train_list = []\n","encoded_X_cal_list   = []\n","encoded_X_test_list  = []\n","\n","for i in range(len(imputed_X_train_list)):\n","    df_train_imp = imputed_X_train_list[i].copy()\n","    df_cal_imp   = imputed_X_cal_list[i].copy()\n","    df_test_imp  = imputed_X_test_list[i].copy()\n","\n","    # 1) Collect all categorical columns present in train_i, cal_i, or test_i\n","    categorical_columns = sorted(\n","        set(df_train_imp.select_dtypes(include=[\"category\", \"object\"]).columns.tolist())\n","        | set(df_cal_imp.select_dtypes(include=[\"category\", \"object\"]).columns.tolist())\n","        | set(df_test_imp.select_dtypes(include=[\"category\", \"object\"]).columns.tolist())\n","    )\n","    print(f\"Imputation {i}: found {len(categorical_columns)} categorical columns to encode.\")\n","\n","    # 2) Concatenate train_i + cal_i + test_i so that get_dummies runs once.\n","    df_combined = pd.concat([df_train_imp, df_cal_imp, df_test_imp], axis=0)\n","\n","    # 3) Loop through each categorical column and create dummies (drop or not as before):\n","    df_combined_enc = df_combined.copy()\n","    for col in categorical_columns:\n","        # Determine the actual levels (ignoring NaN)\n","        if isinstance(df_combined_enc[col].dtype, pd.CategoricalDtype):\n","            levels = list(df_combined_enc[col].cat.categories)\n","        else:\n","            levels = sorted(df_combined_enc[col].dropna().unique())\n","        levels = [lvl for lvl in levels if pd.notna(lvl)]\n","\n","        if len(levels) <= 1:\n","            df_combined_enc.drop(columns=[col], inplace=True)\n","            continue\n","\n","        if len(levels) == 2:\n","            # Binary: drop_first=True → one dummy column\n","            dummies = pd.get_dummies(df_combined_enc[col], prefix=col, drop_first=True)\n","            df_combined_enc = pd.concat(\n","                [df_combined_enc.drop(columns=[col]), dummies],\n","                axis=1\n","            )\n","        else:\n","            # Multi‐level: drop_first=False → full set of dummies\n","            dummies = pd.get_dummies(df_combined_enc[col], prefix=col, drop_first=False)\n","            df_combined_enc = pd.concat(\n","                [df_combined_enc.drop(columns=[col]), dummies],\n","                axis=1\n","            )\n","\n","    # 4) Split the encoded combined back into train_enc_i, cal_enc_i, test_enc_i by index\n","    df_train_enc_i = df_combined_enc.loc[df_train_imp.index].copy()\n","    df_cal_enc_i   = df_combined_enc.loc[df_cal_imp.index].copy()\n","    df_test_enc_i  = df_combined_enc.loc[df_test_imp.index].copy()\n","\n","    # 5) Verify that train_enc_i, cal_enc_i, and test_enc_i share the exact same columns:\n","    cols_train = set(df_train_enc_i.columns)\n","    cols_cal   = set(df_cal_enc_i.columns)\n","    cols_test  = set(df_test_enc_i.columns)\n","    if not (cols_train == cols_cal == cols_test):\n","        raise ValueError(f\"Column mismatch in imputation {i} after dummy encoding!\")\n","    else:\n","        print(f\"Imputation {i}: train/cal/test columns match (n_cols={len(cols_train)})\")\n","\n","    encoded_X_train_list.append(df_train_enc_i)\n","    encoded_X_cal_list.append(df_cal_enc_i)\n","    encoded_X_test_list.append(df_test_enc_i)\n","\n","print(\"Completed one‐hot encoding for all 3 imputed sets.\")\n"]},{"cell_type":"code","execution_count":null,"id":"t-osr_CGLG_O","metadata":{"id":"t-osr_CGLG_O"},"outputs":[],"source":["# ──────────────────────────────────────────────────────────────────────────────\n","# Sanity‐check C (revised): Check no missing values and preview encoded_X_train_list / encoded_X_test_list\n","# ──────────────────────────────────────────────────────────────────────────────\n","\n","import pandas as pd\n","pd.set_option(\"display.max_columns\", None)\n","\n","print(\"Checking for missing values in each one‐hot–encoded TRAIN_FOR_IMPUTE:\")\n","for i, X_enc in enumerate(encoded_X_train_list):\n","    total_na = X_enc.isnull().sum().sum()\n","    status = \"No missing\" if total_na == 0 else f\" {total_na} missing\"\n","    print(f\"  Encoded TRAIN_FOR_IMPUTE {i}: {status}\")\n","\n","print(\"\\nChecking for missing values in each one‐hot–encoded TEST:\")\n","for i, X_enc in enumerate(encoded_X_test_list):\n","    total_na = X_enc.isnull().sum().sum()\n","    status = \"No missing\" if total_na == 0 else f\"{total_na} missing\"\n","    print(f\"  Encoded TEST {i}: {status}\")\n","\n","# ──────────────────────────────────────────────────────────────────────────────\n","# Preview a few rows from each encoded DataFrame\n","# ──────────────────────────────────────────────────────────────────────────────\n","\n","for i, X_enc in enumerate(encoded_X_train_list):\n","    print(f\"\\nEncoded TRAIN_FOR_IMPUTE {i} (preview):\")\n","    display(X_enc.head())\n","    print(\"─\" * 80)\n","\n","for i, X_enc in enumerate(encoded_X_test_list):\n","    print(f\"\\nEncoded TEST {i} (preview):\")\n","    display(X_enc.head())\n","    print(\"─\" * 80)\n"]},{"cell_type":"code","execution_count":null,"id":"HNG8ti0yXrK6","metadata":{"id":"HNG8ti0yXrK6"},"outputs":[],"source":["##check indices match\n","\n","for i in range(len(encoded_X_train_list)):\n","    Xtr = encoded_X_train_list[i]\n","    Xcal = encoded_X_cal_list[i]\n","    Xte = encoded_X_test_list[i]\n","\n","    # Check that Xtr.index == Y_train_for_impute.index\n","    if not Xtr.index.equals(Y_train_for_impute.index):\n","        print(f\"Imputation {i}: index mismatch between X_train_enc and Y_train_for_impute!\")\n","    else:\n","        print(f\"Imputation {i}: X_train_enc index matches Y_train_for_impute.\")\n","\n","    # Check that Xcal.index == Y_cal.index\n","    if not Xcal.index.equals(Y_cal.index):\n","        print(f\"Imputation {i}: index mismatch between X_cal_enc and Y_cal!\")\n","    else:\n","        print(f\"Imputation {i}: X_cal_enc index matches Y_cal.\")\n","\n","    # Check that Xte.index == Y_test.index\n","    if not Xte.index.equals(Y_test.index):\n","        print(f\"Imputation {i}: index mismatch between X_test_enc and Y_test!\")\n","    else:\n","        print(f\"Imputation {i}: X_test_enc index matches Y_test.\")\n","\n","    print(f\"   Shapes: X_train_enc_{i}={Xtr.shape}, X_cal_enc_{i}={Xcal.shape}, X_test_enc_{i}={Xte.shape}\")\n","    print(f\"           Y_train_for_impute={Y_train_for_impute.shape}, Y_cal={Y_cal.shape}, Y_test={Y_test.shape}\\n\")\n"]},{"cell_type":"code","execution_count":null,"id":"8MOgOQv-XrOS","metadata":{"id":"8MOgOQv-XrOS"},"outputs":[],"source":["##now scale continuous variables\n","\n","X_train_s_list = []\n","X_cal_s_list   = []\n","X_test_s_list  = []\n","\n","for i in range(len(encoded_X_train_list)):\n","    Xtr = encoded_X_train_list[i].copy()\n","    Xcal = encoded_X_cal_list[i].copy()\n","    Xte = encoded_X_test_list[i].copy()\n","\n","    # Identify continuous columns (from your original continuous_vars)\n","    continuous_cols = [c for c in continuous_vars if c in Xtr.columns]\n","\n","    if len(continuous_cols) > 0:\n","        # ── a) Cast those columns to float64 on all three splits ─────────────────\n","        Xtr[continuous_cols] = Xtr[continuous_cols].astype(np.float64)\n","        Xcal[continuous_cols] = Xcal[continuous_cols].astype(np.float64)\n","        Xte[continuous_cols] = Xte[continuous_cols].astype(np.float64)\n","\n","        # ── b) Fit StandardScaler on Xtr[continuous_cols]\n","        scaler = StandardScaler()\n","        scaler.fit(Xtr[continuous_cols])\n","\n","        # ── c) Transform all three splits\n","        Xtr_scaled_vals = scaler.transform(Xtr[continuous_cols])\n","        Xcal_scaled_vals = scaler.transform(Xcal[continuous_cols])\n","        Xte_scaled_vals = scaler.transform(Xte[continuous_cols])\n","\n","        # ── d) Now assign back into the DataFrames (no dtype conflict)\n","        Xtr.loc[:, continuous_cols] = Xtr_scaled_vals\n","        Xcal.loc[:, continuous_cols] = Xcal_scaled_vals\n","        Xte.loc[:, continuous_cols] = Xte_scaled_vals\n","\n","    X_train_s_list.append(Xtr)\n","    X_cal_s_list.append(Xcal)\n","    X_test_s_list.append(Xte)\n","    print(f\"Imputation {i}: scaled continuous cols.   (X_train_s_{i}.shape={Xtr.shape})\")\n","\n","print(\"All imputed train/cal/test sets are now scaled.\")\n"]},{"cell_type":"code","execution_count":null,"id":"NEdMASc67y4f","metadata":{"id":"NEdMASc67y4f"},"outputs":[],"source":["##verify shape of each df is as expected\n","\n","for i in range(len(X_train_s_list)):\n","    print(f\"--- Imputation {i} ---\")\n","    print(f\"X_train_s_{i}.shape = {X_train_s_list[i].shape}   (Y_train_for_impute.shape = {Y_train_for_impute.shape})\")\n","    print(f\"X_cal_s_{i}.shape   = {X_cal_s_list[i].shape}   (Y_cal.shape = {Y_cal.shape})\")\n","    print(f\"X_test_s_{i}.shape  = {X_test_s_list[i].shape}   (Y_test.shape = {Y_test.shape})\\n\")\n"]},{"cell_type":"code","execution_count":null,"id":"Ao0h7m17_sWF","metadata":{"id":"Ao0h7m17_sWF"},"outputs":[],"source":["##create a dictionary of model hyper-parameter(s)\n","\n","##for KNN\n","n_list=np.arange(1, 503, 2)\n","param_grid_knc = {'n_neighbors':n_list}\n","\n","##for RF\n","param_grid_rf = {\n","    'n_estimators': [100, 200, 400],       ## Number of trees in the forest\n","    'max_depth': [None, 10, 20, 30],       ## Maximum depth of the trees\n","    'min_samples_split': [2, 5, 10],       ## Minimum number of samples required to split an internal node\n","    'min_samples_leaf': [1, 2, 4],         ## Minimum number of samples required to be at a leaf node\n","    'max_features': ['sqrt']               ## Number of features to consider for the best split\n","}\n","\n","##for LR\n","param_grid_lr = {\n","    'C': [0.001, 0.01, 0.1, 1, 10, 100],   ## Inverse of regularization strength\n","    'penalty': ['l1', 'l2'],               ## Regularization type\n","    'solver': ['liblinear', 'saga'],       ## Optimization algorithm\n","    'max_iter': [100, 200, 300]            ## Maximum number of iterations\n","    }\n","\n","##this is for XGBoost\n","param_grid_gb = {\n","    'learning_rate': [0.01, 0.05, 0.1],    ## Learning rate\n","    'max_depth': [3, 5, 7],                ## Maximum depth of the trees\n","    'subsample': [0.6, 0.8, 1.0],          ## Subsample ratio of the training instances\n","    'colsample_bytree': [0.6, 0.8, 1.0],   ## Subsample ratio of columns when constructing each tree.\n","    'n_estimators': [100, 150, 200]        ## Number of trees\n","}\n"]},{"cell_type":"code","execution_count":null,"id":"rhL6c61376YO","metadata":{"id":"rhL6c61376YO"},"outputs":[],"source":["##XGBoost Hyperparameter optimization\n","# ──────────────────────────────────────────────────────────────\n","# Step 1: Make a safe copy of your training data\n","# ──────────────────────────────────────────────────────────────\n","X_train_s_list_xgb = [\n","    df.copy().set_axis([f\"feat_{j}\" for j in range(df.shape[1])], axis=1)\n","    for df in X_train_s_list\n","]\n","\n","# Build a list of label‐arrays (one per imputation) so pooled_cv_auc can iterate correctly\n","Y_train_list = [Y_train_for_impute.values for _ in range(len(X_train_s_list_xgb))]\n","\n","# ──────────────────────────────────────────────────────────────\n","# Step 2: Define pooled CV‐AUC function\n","# ──────────────────────────────────────────────────────────────\n","def pooled_cv_auc(params_dict, X_list, Y_list, cv_folds=5, random_state=0):\n","    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n","    per_imputation_scores = []\n","\n","    for X_imp, y_imp in zip(X_list, Y_list):\n","        model = xgb.XGBClassifier(\n","            **params_dict,\n","            use_label_encoder=False,\n","            eval_metric=\"logloss\",\n","            random_state=random_state\n","        )\n","        scores = cross_val_score(\n","            model,\n","            X_imp,\n","            y_imp,\n","            cv=skf,\n","            scoring=\"roc_auc\",\n","            n_jobs=-1\n","        )\n","        per_imputation_scores.append(np.mean(scores))\n","\n","    return float(np.mean(per_imputation_scores))\n","\n","# ──────────────────────────────────────────────────────────────\n","# Step 3: Run grid search on the safe XGBoost copy\n","# ──────────────────────────────────────────────────────────────\n","results = []\n","\n","for candidate_params in ParameterGrid(param_grid_gb):\n","    pooled_auc = pooled_cv_auc(\n","        candidate_params,\n","        X_train_s_list_xgb,\n","        Y_train_list,\n","        cv_folds=5,\n","        random_state=0\n","    )\n","    results.append((candidate_params, pooled_auc))\n","\n","df_results = pd.DataFrame([\n","    {**params, \"pooled_cv_auc\": auc_score}\n","    for (params, auc_score) in results\n","]).sort_values(\"pooled_cv_auc\", ascending=False).reset_index(drop=True)\n","\n","print(\"\\nTop 5 hyperparameter combinations (by pooled CV‐AUC):\")\n","print(df_results.head(5).to_string(index=False, float_format=\"{:.4f}\".format))\n","\n","best_params = df_results.iloc[0].drop(\"pooled_cv_auc\").to_dict()\n","print(f\"\\n>>> Best parameters (pooled across imputations):\\n{best_params}\")\n"]},{"cell_type":"code","execution_count":null,"id":"_kfDTdpP_GKm","metadata":{"id":"_kfDTdpP_GKm"},"outputs":[],"source":["##L1/L2 LR hyperparameter optimization\n","# ───────────────────────────────────────────────────────────────────────────────\n","# Helper: compute pooled CV‐AUC for LogisticRegression\n","# ───────────────────────────────────────────────────────────────────────────────\n","def pooled_cv_auc_lr(params_dict, X_list, Y_list, cv_folds=5, random_state=0):\n","    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n","    per_imputation_scores = []\n","\n","    for X_imp, y_imp in zip(X_list, Y_list):\n","        model = LogisticRegression(\n","            **params_dict,\n","            random_state=0,\n","            verbose=2\n","        )\n","        scores = cross_val_score(\n","            model,\n","            X_imp,\n","            y_imp,\n","            cv=skf,\n","            scoring=\"roc_auc\",\n","            n_jobs=-1\n","        )\n","        per_imputation_scores.append(np.mean(scores))\n","\n","    return float(np.mean(per_imputation_scores))\n","\n","# ──────────────────────────────────────────────────────────────\n","# Step 1: Make a safe copy of your training data\n","# ──────────────────────────────────────────────────────────────\n","X_train_s_list_lr = [\n","    df.copy().set_axis([f\"feat_{j}\" for j in range(df.shape[1])], axis=1)\n","    for df in X_train_s_list]\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# Step 1: Build Y_train_list (one array per imputation), if you haven’t already\n","# ───────────────────────────────────────────────────────────────────────────────\n","# (This is exactly the same trick we used for XGBoost.)\n","Y_train_list = [Y_train_for_impute.values for _ in range(len(X_train_s_list_lr))]\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# Step 2: Grid search for Logistic Regression\n","# ───────────────────────────────────────────────────────────────────────────────\n","results_lr = []\n","\n","for candidate_params in ParameterGrid(param_grid_lr):\n","    pooled_auc = pooled_cv_auc_lr(\n","        candidate_params,\n","        X_train_s_list_lr,\n","        Y_train_list,\n","        cv_folds=5,\n","        random_state=0\n","    )\n","    results_lr.append((candidate_params, pooled_auc))\n","\n","# Convert to DataFrame and sort\n","df_results_lr = pd.DataFrame([\n","    {**params, \"pooled_cv_auc\": auc_score}\n","    for (params, auc_score) in results_lr\n","]).sort_values(\"pooled_cv_auc\", ascending=False).reset_index(drop=True)\n","\n","print(\"\\nTop 5 Logistic Regression hyperparameter combinations (by pooled CV‐AUC):\")\n","print(df_results_lr.head(5).to_string(index=False, float_format=\"{:.4f}\".format))\n","\n","best_params_lr = df_results_lr.iloc[0].drop(\"pooled_cv_auc\").to_dict()\n","print(f\"\\n>>> Best Logistic Regression parameters (pooled across imputations):\\n{best_params_lr}\")\n"]},{"cell_type":"code","execution_count":null,"id":"OmXQ06RgBKGT","metadata":{"id":"OmXQ06RgBKGT"},"outputs":[],"source":["##RF hyperparameter optimization\n","# ──────────────────────────────────────────────────────────────\n","# Step 1: Make a safe copy of your training data\n","# ──────────────────────────────────────────────────────────────\n","X_train_s_list_rf = [\n","    df.copy().set_axis([f\"feat_{j}\" for j in range(df.shape[1])], axis=1)\n","    for df in X_train_s_list]\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 1) Build Y_train_list (one array per imputation), if not already created\n","# ───────────────────────────────────────────────────────────────────────────────\n","Y_train_list = [Y_train_for_impute.values for _ in range(len(X_train_s_list_rf))]\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 2) Define pooled CV‐AUC function for Random Forest\n","# ───────────────────────────────────────────────────────────────────────────────\n","def pooled_cv_auc_rf(params_dict, X_list, Y_list, cv_folds=5, random_state=0):\n","    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=random_state)\n","    per_imputation_scores = []\n","\n","    for X_imp, y_imp in zip(X_list, Y_list):\n","        model = RandomForestClassifier(\n","            **params_dict,\n","            random_state=0,\n","            n_jobs=-1\n","        )\n","        scores = cross_val_score(\n","            model,\n","            X_imp,\n","            y_imp,\n","            cv=skf,\n","            scoring=\"roc_auc\",\n","            n_jobs=-1\n","        )\n","        per_imputation_scores.append(np.mean(scores))\n","\n","    return float(np.mean(per_imputation_scores))\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 3) Run pooled grid search across imputations (using calibration splits)\n","# ───────────────────────────────────────────────────────────────────────────────\n","results_rf = []\n","\n","for candidate_params in ParameterGrid(param_grid_rf):\n","    pooled_auc = pooled_cv_auc_rf(\n","        candidate_params,\n","        X_train_s_list_rf,\n","        Y_train_list,\n","        cv_folds=5,\n","        random_state=0\n","    )\n","    results_rf.append((candidate_params, pooled_auc))\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 4) Store results in DataFrame and identify best parameters\n","# ───────────────────────────────────────────────────────────────────────────────\n","df_results_rf = pd.DataFrame([\n","    {**params, \"pooled_cv_auc\": auc_score}\n","    for (params, auc_score) in results_rf\n","]).sort_values(\"pooled_cv_auc\", ascending=False).reset_index(drop=True)\n","\n","print(\"\\nTop 5 RF hyperparameter combinations (by pooled CV‐AUC):\")\n","print(df_results_rf.head(5).to_string(index=False, float_format=\"{:.4f}\".format))\n","\n","best_params_rf = df_results_rf.iloc[0].drop(\"pooled_cv_auc\").to_dict()\n","print(f\"\\n>>> Best RF parameters (pooled across imputations):\\n{best_params_rf}\")\n"]},{"cell_type":"code","execution_count":null,"id":"NixdRPxfBKNs","metadata":{"id":"NixdRPxfBKNs"},"outputs":[],"source":["##KNN hyperparameter optimization\n","# ──────────────────────────────────────────────────────────────\n","# Step 1: Make a safe copy of your training data\n","# ──────────────────────────────────────────────────────────────\n","X_train_s_list_knn = [\n","    df.copy().set_axis([f\"feat_{j}\" for j in range(df.shape[1])], axis=1)\n","    for df in X_train_s_list]\n","# ───────────────────────────────────────────────────────────────────────────────\n","# Build Y_train_list (one array per imputation), if not already created\n","# ───────────────────────────────────────────────────────────────────────────────\n","Y_train_list = [Y_train_for_impute.values for _ in range(len(X_train_s_list_knn))]\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 1) Define pooled CV‐AUC function for KNN\n","# ───────────────────────────────────────────────────────────────────────────────\n","def pooled_cv_auc_knn(params_dict, X_list, Y_list, cv_folds=5):\n","    skf = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=0)\n","    per_imputation_scores = []\n","\n","    for X_imp, y_imp in zip(X_list, Y_list):\n","        model = KNeighborsClassifier(**params_dict)\n","        scores = cross_val_score(\n","            model,\n","            X_imp,\n","            y_imp,\n","            cv=skf,\n","            scoring=\"roc_auc\",\n","            n_jobs=-1\n","        )\n","        per_imputation_scores.append(np.mean(scores))\n","\n","    return float(np.mean(per_imputation_scores))\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 2) Run pooled grid search across imputations (using calibration splits)\n","# ───────────────────────────────────────────────────────────────────────────────\n","results_knn = []\n","\n","for candidate_params in ParameterGrid(param_grid_knc):\n","    pooled_auc = pooled_cv_auc_knn(\n","        candidate_params,\n","        X_train_s_list_knn,\n","        Y_train_list,\n","        cv_folds=5\n","    )\n","    results_knn.append((candidate_params, pooled_auc))\n","\n","# ───────────────────────────────────────────────────────────────────────────────\n","# 3) Store results in DataFrame and identify best parameters\n","# ───────────────────────────────────────────────────────────────────────────────\n","df_results_knn = pd.DataFrame([\n","    {**params, \"pooled_cv_auc\": auc_score}\n","    for (params, auc_score) in results_knn\n","]).sort_values(\"pooled_cv_auc\", ascending=False).reset_index(drop=True)\n","\n","print(\"\\nTop 5 KNN hyperparameter combinations (by pooled CV‐AUC):\")\n","print(df_results_knn.head(5).to_string(index=False, float_format=\"{:.4f}\".format))\n","\n","best_params_knn = df_results_knn.iloc[0].drop(\"pooled_cv_auc\").to_dict()\n","print(f\"\\n>>> Best KNN parameters (pooled across imputations):\\n{best_params_knn}\")\n"]},{"cell_type":"code","execution_count":null,"id":"edX915cqBdl3","metadata":{"id":"edX915cqBdl3"},"outputs":[],"source":["##prepare to run neural networks to test which DeepTable arhcitecture performs best\n","\n","!pip install deeptables\n","##revert to sklearn 1.5 to resolve dependency issues\n","!pip install scikit-learn==1.5\n","import deeptables\n","print(\"dt version:\", deeptables.__version__)\n","from deeptables.models.deeptable import DeepTable, ModelConfig\n","from deeptables.models.deepnets import DeepFM, WideDeep, DCN"]},{"cell_type":"code","execution_count":null,"id":"RA2iktVOBdr4","metadata":{"id":"RA2iktVOBdr4"},"outputs":[],"source":["# ───────────────────────────────────────────────────────────────────────────────\n","# Build label lists (one per imputation) for calibration and test sets\n","# ───────────────────────────────────────────────────────────────────────────────\n","Y_cal_list  = [Y_cal.values for _ in range(len(X_cal_s_list))]\n","Y_test_list = [Y_test.values for _ in range(len(X_test_s_list))]\n","\n","# ──────────────────────────────────────────────────────────────\n","# Helper to clean and align DataFrames\n","# ──────────────────────────────────────────────────────────────\n","def clean_df(df, ref_columns=None):\n","    df = df.copy()\n","    for col in df.columns:\n","        if df[col].dtype == bool:\n","            df[col] = df[col].astype(int)\n","    if ref_columns is not None:\n","        missing_cols = [c for c in ref_columns if c not in df.columns]\n","        for col in missing_cols:\n","            df[col] = 0\n","        df = df[ref_columns]\n","    return df\n","\n","# ──────────────────────────────────────────────────────────────\n","# Define architectures to test\n","# ──────────────────────────────────────────────────────────────\n","architectures = {\"DeepFM\": DeepFM, \"DCN\": DCN, \"WideDeep\": WideDeep}\n","results = {}\n","\n","# ──────────────────────────────────────────────────────────────\n","# Loop through architectures and imputations\n","# ──────────────────────────────────────────────────────────────\n","for arch_name, arch_class in architectures.items():\n","    aucs = []\n","\n","    for i in range(len(X_train_s_list)):\n","        # 1) Training split for model fitting\n","        X_train_df = clean_df(X_train_s_list[i])\n","        Y_train_s  = Y_train_for_impute.values  # same labels for all imputations\n","\n","        # 2) Validation (calibration) split\n","        X_val_df = clean_df(\n","            X_cal_s_list[i],\n","            ref_columns=X_train_df.columns\n","        )\n","        Y_val_s = Y_cal_list[i]\n","\n","        # 3) Test split\n","        X_test_df = clean_df(\n","            X_test_s_list[i],\n","            ref_columns=X_train_df.columns\n","        )\n","        Y_test_s = Y_test_list[i]\n","\n","        # 4) Build model config\n","        conf = ModelConfig(\n","            nets=arch_class,\n","            metrics=[\"AUC\", \"accuracy\"],\n","            auto_discrete=True,\n","            auto_imputation=False,\n","            earlystopping_patience=5\n","        )\n","\n","        # 5) Instantiate DeepTable and fit\n","        dt = DeepTable(config=conf)\n","        model, history = dt.fit(\n","            X_train_df,\n","            Y_train_s,\n","            epochs=100,\n","            validation_data=(X_val_df, Y_val_s)\n","        )\n","\n","        # 6) Predict on test and compute AUC\n","        y_pred_prob = dt.predict_proba(X_test_df)[:, 1]\n","        auc_score = roc_auc_score(Y_test_s, y_pred_prob)\n","        aucs.append(auc_score)\n","\n","    results[arch_name] = np.mean(aucs)\n","\n","# ──────────────────────────────────────────────────────────────\n","# Output summary\n","# ──────────────────────────────────────────────────────────────\n","print(\"\\nArchitecture Comparison (Average AUC across imputations):\")\n","for name, score in results.items():\n","    print(f\"{name:<10}: {score:.4f}\")\n","\n","best_model = max(results, key=results.get)\n","print(f\"\\nBest architecture: {best_model}\")\n"]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[{"file_id":"1DFrM2kQIaxxoreKKfMJY8bZ75pnIBOBL","timestamp":1734555115141},{"file_id":"1XL3SGHIBPO06uNNL8pc4jbgyODiCHYmT","timestamp":1734454404403},{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","##due to potential module dependencies, we will install DeepTables later\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","!pip install shap\n","import shap\n","import sys\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import seaborn as sn\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc, precision_recall_curve, recall_score, confusion_matrix, brier_score_loss\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import resample\n","sn.set(style='whitegrid')\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)\n","print(\"shap version:\", shap.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["##import your dataset\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["# Import data and specify missing values\n","data = pd.read_csv(file_path, na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'])\n","\n","# Filter out rows where 'TRAUMATYPE' is 26, 'Other/unspecified', or 'Burn'\n","try:\n","  exclude_values = ['26', 'Other/unspecified', 'Burn']\n","  data = data[~data['TRAUMATYPE'].isin(exclude_values)]\n","except:\n","  pass\n","\n","\n","##we must test on the same dataset for paired bootstrap testing, so any cases that do not have a predicted\n","##probability based on logisitc regression (those cases with mising values for predictors) will need to be\n","##ecdluded\n","required_vars = ['PRE_3']\n","data = data.dropna(subset=required_vars)\n","\n","##reset indices of the df\n","data.reset_index(drop=True, inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##check dataframe to ensure it appears as it should\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing data\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a dataframe of all complications/things not available on admission.  We can remove all of these from the X data set and pick one to be\n","#our Y dataset\n","\n","complications_df=pd.DataFrame()\n","complications_list= [\n","                    'HC_CLABSI', 'HC_DEEPSSI', 'HC_DVTHROMBOSIS', 'HC_ALCOHOLWITHDRAWAL', 'HC_CARDARREST', 'HC_CAUTI',\n","                    'HC_EMBOLISM', 'HC_EXTREMITYCS', 'HC_INTUBATION', 'HC_KIDNEY', 'HC_MI', 'HC_ORGANSPACESSI',\n","                    'HC_OSTEOMYELITIS', 'HC_RESPIRATORY', 'HC_RETURNOR', 'HC_SEPSIS', 'HC_STROKECVA', 'HC_SUPERFICIALINCISIONSSI',\n","                    'HC_PRESSUREULCER', 'HC_UNPLANNEDICU', 'HC_VAPNEUMONIA',\n","                    ##'EDDISCHARGEDISPOSITION',\n","                    'HOSPDISCHARGEDISPOSITION',\n","                    ##'EDDISCHARGEHRS',\n","                    'WITHDRAWALLST',\n","                    'VTEPROPHYLAXISTYPE',\n","                    'TOTALICULOS',\n","                    'TOTALVENTDAYS',\n","                    'VTEPROPHYLAXISHRS',\n","                    'VTEPROPHYLAXISDAYS', 'MORTALITY', 'EDDISCHARGEDAYS','FINALDISCHARGEDAYS','FINALDISCHARGEHRS', 'HMRRHGCTRLSURGDAYS',  'WITHDRAWALLSTHRS',\n","                    ##'AMERICANINDIAN', 'ASIAN', 'BLACK', 'PACIFICISLANDER', 'RACEOTHER', 'WHITE', 'RACE_NA', 'RACE_UK',\n","                    'ISS_05'\n","                    , 'AIS_FACE', 'AIS_NECK', 'AIS_HEAD', 'AIS_THORAX', 'AIS_ABDOMEN', 'AIS_SPINE', 'AIS_UPPEREX', 'AIS_LOWEREX', 'AIS_SKIN', 'AIS_OTHER'\n","                    , 'VTEPPXStartOver48', 'VTEPPXStartOver24', 'ICUOver48', 'ICUOver24', 'VentOver48', 'VentOver24'\n","                    , 'VTEPPXStartOver72', 'VTEPPXStartOver96', 'ICUOver72', 'ICUOver96', 'VentOver72', 'VentOver96'\n","                    , 'FacilityTotalWLST', 'factilityTotalPatients', 'FacilityWLSTRate'\n","                    , 'facilityWLSTNew', 'WLSTRateNew', 'WLSTRateNewCensored'\n","                    , 'WLSTRateCensorNormal', 'facilityPatientsNew', 'FacilityKey'\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, in this case, WLST, and move it to a separate dataframe\n","Y_data = pd.DataFrame()\n","Y_data['WLST'] = data['WITHDRAWALLST']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","\n","Y_data['WLST'] = Y_data['WLST'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##remove all unwanted variables as defined above from the input space\n","X_data = data.drop(columns=complications_list)\n","X_data.shape"]},{"cell_type":"code","source":["##need to remove any cases with missing data for our outcome variable\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"],"metadata":{"id":"eh992q4ujyp7"},"id":"eh992q4ujyp7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##here we find which rows in Y have missing values\n","\n","bad_row_index_list=[]\n","for n in range(0, Y_data.shape[0]):\n","    n_missings=Y_data.iloc[n,:].isnull().sum()\n","    if n_missings>0:\n","        bad_row_index_list.append(n)\n","bad_row_index_list"],"metadata":{"id":"73arQ_SPj1Iy"},"id":"73arQ_SPj1Iy","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b9270983","metadata":{"id":"b9270983"},"outputs":[],"source":["##now remove the bad rows in Y\n","Y_clean = Y_data.drop(bad_row_index_list, axis=0)\n","Y_clean"]},{"cell_type":"code","source":["##ensure all cases with missing values for the outcome have been dropped\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"],"metadata":{"id":"ZWG9Tdo_gk5e"},"id":"ZWG9Tdo_gk5e","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##and remove bad rows in X\n","X_data=X_data.drop(bad_row_index_list, axis=0)"],"metadata":{"id":"M4GwsrAqj_r2"},"id":"M4GwsrAqj_r2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##order variables with missing data by percentage\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##display variables withOUT mising data\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["#remove the good columns (no missing values) from data_missing\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##actually drop bad variables\n","X_data_new=X_data.drop(columns=bad_column_names, axis=1)\n","\n","##check for which variables still have missing data (<50% missing values)\n","Missing = X_data_new.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["#check for columns with less than 50% missing that need to be cleaned\n","to_be_cleaned_column_names = data_missing[data_missing <50].index\n","to_be_cleaned_column_names"]},{"cell_type":"code","execution_count":null,"id":"b5ec271e","metadata":{"id":"b5ec271e"},"outputs":[],"source":["##perform median imputation for continuous variable and mode imputation for categorical\n","for c in to_be_cleaned_column_names:\n","    v=X_data_new[c]#get values in this column\n","    v_valid=v[~v.isnull()] # get valid values\n","    if X_data_new[c].dtype == np.dtype('O'): # non-numeric values\n","        X_data_new[c]=X_data_new[c].fillna(v.value_counts().index[0]).astype(object) # the most frequent category\n","    else: # numeric\n","        X_data_new[c]=X_data_new[c].fillna(v_valid.median()) #replace nan with median value"]},{"cell_type":"code","execution_count":null,"id":"31a5eafa","metadata":{"id":"31a5eafa"},"outputs":[],"source":["##confirm no more missing data in imput space\n","X_data_new.isnull().sum().sum()"]},{"cell_type":"code","execution_count":null,"id":"0a109d01","metadata":{"id":"0a109d01"},"outputs":[],"source":["##verify cleaned dataframe appears as intended\n","X_data_new.head()"]},{"cell_type":"code","execution_count":null,"id":"iWR41vor-_Pu","metadata":{"id":"iWR41vor-_Pu"},"outputs":[],"source":["# Rename the 'TRAUMATYPE' column to 'Penetrating' and map the values to 0 and 1\n","X_data_new['Penetrating'] = X_data_new['TRAUMATYPE'].map({'Penetrating': 1, 'Blunt': 0})\n","\n","# Drop the old 'TRAUMATYPE' column\n","X_data_new.drop(columns=['TRAUMATYPE'], inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"c14955b3","metadata":{"id":"c14955b3"},"outputs":[],"source":["# Display the entire DataFrame without truncation\n","pd.set_option('display.max_columns', None)\n","\n","# Get column names and data types\n","columns_info = []\n","for column_name, dtype in zip(X_data_new.columns, X_data_new.dtypes):\n","    columns_info.append(f\"{column_name}: {dtype}\")\n","\n","formatted_columns_info = \"\\n\".join(columns_info)\n","\n","# Print column names and data types\n","print(\"Column Names and Data Types:\")\n","print(formatted_columns_info)"]},{"cell_type":"code","source":["##remove any additional variables necessary\n","## Remove the \"RACE\" and \"TRANSPORTMODE\" columns, as these are composite varibles that have already been 1 hot encoded\n","columns_to_remove = ['RACE', 'TRANSPORTMODE']\n","X_data_new = X_data_new.drop(columns=columns_to_remove, errors='ignore')"],"metadata":{"id":"aHrlRyyIkk5V"},"id":"aHrlRyyIkk5V","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2727c717","metadata":{"id":"2727c717"},"outputs":[],"source":["##first we will convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","##want code to be reusable between different populations of input data.  Not every population will have all of these variables\n","##Therefore, will do everything within separate try/except blocks\n","\n","try:\n","    X_data_new= X_data_new.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","try:\n","    X_data_new['ETHNICITY'] = X_data_new['ETHNICITY'].replace({'Hispanic or Latino': 1, 'Not Hispanic or Latino': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSEYE'] = X_data_new['EMSGCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3,\n","                                                               'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSEYE'] = X_data_new['GCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3, 'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSVERBAL'] = X_data_new['EMSGCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                                     'Confused': 4, 'Oriented': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSMOTOR'] = X_data_new['EMSGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['TBIGCSMOTOR'] = X_data_new['TBIGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSVERBAL'] = X_data_new['GCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                               'Confused': 4, 'Orientated': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSMOTOR'] = X_data_new['GCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                           'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['RESPIRATORYASSISTANCE'] = X_data_new['RESPIRATORYASSISTANCE'].replace({'Assisted Respiratory Rate': 1,\n","                                                                                   'Unassisted Respiratory Rate': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['SUPPLEMENTALOXYGEN'] = X_data_new['SUPPLEMENTALOXYGEN'].replace({'Supplemental Oxygen': 1,\n","                                                                             'No Supplemental Oxygen': 0})\n","except:\n","    pass\n","\n","X_data_new.head()\n","\n","##male coded as 0\n","##female coded as 1\n","\n","##not hispanic coded as 0\n","##hispanic coded as 1"]},{"cell_type":"code","execution_count":null,"id":"3f17738d","metadata":{"id":"3f17738d"},"outputs":[],"source":["##need to convert categorical values to numerical values using one-hot encoding\n","categorical_column=[]\n","for c in X_data_new.columns:\n","    if X_data_new[c].dtype == np.dtype('O', 'category'): # non-numeric values\n","        categorical_column.append(c)\n","categorical_column"]},{"cell_type":"code","execution_count":null,"id":"2770257a","metadata":{"id":"2770257a"},"outputs":[],"source":["##check how many variables we need to one-hot encode\n","len(categorical_column)"]},{"cell_type":"code","execution_count":null,"id":"60e3b4bf","metadata":{"id":"60e3b4bf"},"outputs":[],"source":["##verify dataframe shape\n","X_data_new.shape"]},{"cell_type":"code","execution_count":null,"id":"323357b1","metadata":{"id":"323357b1"},"outputs":[],"source":["##one-hot encode variables above\n","X_clean=pd.get_dummies(X_data_new, columns=categorical_column, sparse=False)\n","X_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"c524ec1d","metadata":{"id":"c524ec1d"},"outputs":[],"source":["##verify cleaned true label dataframe shape\n","Y_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"44d987e0","metadata":{"id":"44d987e0"},"outputs":[],"source":["##verify no missing data in the cleaned input space\n","X_clean.isnull().sum().sum()"]},{"cell_type":"code","source":["##drop patient ID's\n","X_clean.drop(['inc_key'], axis=1, inplace=True)"],"metadata":{"id":"Qc1yYzaQk2T8"},"id":"Qc1yYzaQk2T8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"630434e8","metadata":{"id":"630434e8"},"outputs":[],"source":["##replace boolean values in binary variables to numeric values\n","X_clean = X_clean.replace({True: 1, False: 0})"]},{"cell_type":"code","execution_count":null,"id":"mrWz__0NRJtw","metadata":{"id":"mrWz__0NRJtw"},"outputs":[],"source":["##verify dataframe appears as intended\n","X_clean.head()"]},{"cell_type":"code","execution_count":null,"id":"AbyaxTByRK__","metadata":{"id":"AbyaxTByRK__"},"outputs":[],"source":["##split cleaned input space into training and test sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X_clean, Y_clean, test_size=0.2, random_state=0)"]},{"cell_type":"code","source":["##further split the training set into a training and validation/calibration set\n","X_train_cal, X_val_cal, Y_train_cal, Y_val_cal = train_test_split(X_train, Y_train, test_size=0.2, random_state=0)"],"metadata":{"id":"fnKaungA3f7m"},"id":"fnKaungA3f7m","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##Prepare to drop the predicted probability calculated by logisitic regression from the input space\n","new_to_drop = ['PRE_3']\n","\n","##create new dataframe just containing this predicted probability\n","X_LR=pd.DataFrame()\n","X_LR['PRE_3']=X_test['PRE_3']"],"metadata":{"id":"6GqABTYP3g0t"},"id":"6GqABTYP3g0t","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##drop the predicted probability from the input dataframes\n","X_train_cal.drop(columns=new_to_drop, inplace=True)\n","X_test.drop(columns=new_to_drop, inplace=True)\n","X_val_cal.drop(columns=new_to_drop, inplace=True)\n","\n","\n","##copy input dataframes to tensor format, if needed\n","X_train_tensor=X_train_cal.copy()\n","Y_train_tensor=Y_train_cal.copy()\n","\n","X_val_tensor=X_val_cal.copy()\n","Y_val_tensor=Y_val_cal.copy()\n","\n","X_test_tensor=X_test.copy()\n","Y_test_tensor=Y_test.copy()"],"metadata":{"id":"9ZIfcWot3vSf"},"id":"9ZIfcWot3vSf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##Lets normalize the input space data\n","\n","scaler=StandardScaler()\n","#get the parameters of the transform\n","scaler.fit(X_train_cal)\n","#normalize the features in the training set\n","X_train_s_cal = scaler.transform(X_train_cal)\n","#normalize the features in the test set\n","print(\"After train/test split, X_test shape:\", X_test.shape)\n","X_test_s = scaler.transform(X_test)\n","print(\"After scaling, X_test_s shape:\", X_test_s.shape)\n","#normalize the features in the val set\n","X_val_s_cal = scaler.transform(X_val_cal)"],"metadata":{"id":"lZMeFuPW37M_"},"id":"lZMeFuPW37M_","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"vtemq7fdbjoE","metadata":{"id":"vtemq7fdbjoE"},"outputs":[],"source":["##now, initialize XGBoost model using parameters determined from gridsearchCV hyperparameter optimization\n","model_gb=xgb.XGBClassifier(random_state=0)\n","model_best_gb = xgb.XGBClassifier(random_state=0, colsample_bytree=0.6, learning_rate=0.05, max_depth=7, n_estimators=150, subsample=0.8)"]},{"cell_type":"code","execution_count":null,"id":"L9Wj3__pbnO6","metadata":{"id":"L9Wj3__pbnO6"},"outputs":[],"source":["# Get predicted probabilities for the positive class (WLST)\n","model_best_gb.fit(X_train_s_cal, Y_train_cal)\n","y_prob_gbo_mtp = model_best_gb.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo_mtp)\n","print(f\"AUROC on the test set: {auroc_gbo}\")"]},{"cell_type":"code","source":["# Calibrate the model on the validation set\n","calibrated_model = CalibratedClassifierCV(estimator=model_best_gb, method='isotonic', cv='prefit')\n","calibrated_model.fit(X_val_s_cal, Y_val_cal)"],"metadata":{"id":"KzRmMMFYqgJF"},"id":"KzRmMMFYqgJF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get predicted probabilities for the positive class (class 1)\n","y_prob_gbo = calibrated_model.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo)\n","print(f\"AUROC on the test set: {auroc_gbo}\")\n","\n","\n","\n","# Calculate ROC curve\n","y_pred_prob_gbo = calibrated_model.predict_proba(X_test_s)[:, 1]\n","fpr_gbo, tpr_gbo, thresholds = roc_curve(Y_test, y_pred_prob_gbo)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_gbo = auc(fpr_gbo, tpr_gbo)"],"metadata":{"id":"ra0ZhEsu4Yku"},"id":"ra0ZhEsu4Yku","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute AUROC LR\n","auroc_iss = roc_auc_score(Y_test, X_LR)\n","print(f\"AUROC on the test set: {auroc_iss}\")"],"metadata":{"id":"R63T8Gce4cQ4"},"id":"R63T8Gce4cQ4","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##prepare a function for paired bootstrap testing\n","\n","def paired_bootstrap_auc_test(\n","    y_true,\n","    predA,\n","    predB,\n","    n_boot=1000,\n","    alpha=0.05,\n","    random_state=None\n","):\n","    \"\"\"\n","    Compare two correlated AUCs (predA vs. predB) via paired bootstrapping,\n","    also returning 95% CI for each AUC individually.\n","\n","    Parameters\n","    ----------\n","    y_true : array-like of shape (n_samples,)\n","        Ground truth binary labels (0 or 1).\n","    predA : array-like of shape (n_samples,)\n","        Scores (e.g., probabilities) from model/variable A.\n","    predB : array-like of shape (n_samples,)\n","        Scores from model/variable B.\n","    n_boot : int, default=1000\n","        Number of bootstrap iterations.\n","    alpha : float, default=0.05\n","        Significance level (for the (1 - alpha) CIs).\n","    random_state : int or None\n","        Random seed for reproducibility.\n","\n","    Returns\n","    -------\n","    results : dict\n","        Dictionary containing:\n","        - aucA, aucB: AUC on the original dataset for each method.\n","        - aucA_ci_lower, aucA_ci_upper: (1 - alpha) CI for A's AUC.\n","        - aucB_ci_lower, aucB_ci_upper: (1 - alpha) CI for B's AUC.\n","        - baseline_diff: AUC(A) - AUC(B) on the full dataset (no resampling).\n","        - mean_diff: Mean difference of AUCs across bootstrap samples.\n","        - diff_ci_lower, diff_ci_upper: (1 - alpha) CI for AUC difference.\n","        - p_value: Approx. two-sided p-value for difference in AUC.\n","    \"\"\"\n","\n","    # Convert to NumPy arrays\n","    y_true = np.asarray(y_true)\n","    predA = np.asarray(predA)\n","    predB = np.asarray(predB)\n","\n","    # Basic checks\n","    assert len(y_true) == len(predA) == len(predB), \"Arrays must all have the same length.\"\n","    n = len(y_true)\n","\n","    # Compute AUC on the full (original) dataset\n","    aucA = roc_auc_score(y_true, predA)\n","    aucB = roc_auc_score(y_true, predB)\n","    baseline_diff = aucA - aucB\n","\n","    # Random generator\n","    rng = np.random.default_rng(random_state)\n","\n","    # Arrays to store bootstrapped AUCs\n","    aucAs = np.zeros(n_boot)\n","    aucBs = np.zeros(n_boot)\n","    diffs = np.zeros(n_boot)\n","\n","    for i in range(n_boot):\n","        # 1) Sample n indices with replacement\n","        sample_idx = rng.integers(0, n, size=n)\n","\n","        # 2) Create bootstrap samples\n","        y_samp = y_true[sample_idx]\n","        A_samp = predA[sample_idx]\n","        B_samp = predB[sample_idx]\n","\n","        # 3) Compute AUC for each method\n","        aucA_samp = roc_auc_score(y_samp, A_samp)\n","        aucB_samp = roc_auc_score(y_samp, B_samp)\n","\n","        # 4) Store AUCs and difference\n","        aucAs[i] = aucA_samp\n","        aucBs[i] = aucB_samp\n","        diffs[i] = aucA_samp - aucB_samp\n","\n","    # Compute individual AUC CIs\n","    aucA_ci_lower = np.percentile(aucAs, 100 * (alpha / 2))\n","    aucA_ci_upper = np.percentile(aucAs, 100 * (1 - alpha / 2))\n","\n","    aucB_ci_lower = np.percentile(aucBs, 100 * (alpha / 2))\n","    aucB_ci_upper = np.percentile(aucBs, 100 * (1 - alpha / 2))\n","\n","    # Compute difference CI\n","    diff_ci_lower = np.percentile(diffs, 100 * (alpha / 2))\n","    diff_ci_upper = np.percentile(diffs, 100 * (1 - alpha / 2))\n","\n","    # Approx. two-sided p-value by sign test on \"diffs\"\n","    n_neg = np.sum(diffs < 0)\n","    n_pos = np.sum(diffs > 0)\n","    p_val = 2.0 * min(n_neg, n_pos) / n_boot\n","    p_val = min(p_val, 1.0)\n","\n","    coverage = (1 - alpha) * 100.0  # e.g., 95.0 if alpha=0.05\n","\n","    return {\n","        \"aucA\": aucA,\n","        \"aucB\": aucB,\n","        \"aucA_ci_lower\": aucA_ci_lower,\n","        \"aucA_ci_upper\": aucA_ci_upper,\n","        \"aucB_ci_lower\": aucB_ci_lower,\n","        \"aucB_ci_upper\": aucB_ci_upper,\n","        \"baseline_diff\": baseline_diff,\n","        \"mean_diff\": np.mean(diffs),\n","        \"diff_ci_lower\": diff_ci_lower,\n","        \"diff_ci_upper\": diff_ci_upper,\n","        \"p_value\": p_val,\n","        \"coverage\": coverage\n","    }"],"metadata":{"id":"IQfJPM8s4dfP"},"id":"IQfJPM8s4dfP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# actually perform paired bootstrapping comparing ML model to LR model predictions\n","\n","pairs = {\n","    \"LR\": X_LR.values,\n","}\n","\n","for var_name, var_array in pairs.items():\n","    results = paired_bootstrap_auc_test(\n","        y_true=Y_test,\n","        predA=y_prob_gbo_mtp,\n","        predB=var_array,\n","        n_boot=2000,      # or more for higher precision\n","        alpha=(0.05),\n","        random_state=42\n","    )\n","    coverage_str = f\"{results['coverage']:.1f}%\"\n","\n","    print(f\"--- ML Model vs. {var_name} ---\")\n","    print(f\"AUC(ML) = {results['aucA']:.3f}, {coverage_str} CI: \"\n","          f\"[{results['aucA_ci_lower']:.3f}, {results['aucA_ci_upper']:.3f}]\")\n","    print(f\"AUC({var_name}) = {results['aucB']:.3f}, {coverage_str} CI: \"\n","          f\"[{results['aucB_ci_lower']:.3f}, {results['aucB_ci_upper']:.3f}]\")\n","    print(f\"AUC diff (ML - {var_name}) = {results['baseline_diff']:.4f}, {coverage_str} CI: \"\n","          f\"[{results['diff_ci_lower']:.4f}, {results['diff_ci_upper']:.4f}]\")\n","    print(f\"p-value = {results['p_value']:.4f}\\n\")"],"metadata":{"id":"NWgBJ4dh4mg2"},"id":"NWgBJ4dh4mg2","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1DFrM2kQIaxxoreKKfMJY8bZ75pnIBOBL","timestamp":1734555115141},{"file_id":"1XL3SGHIBPO06uNNL8pc4jbgyODiCHYmT","timestamp":1734454404403},{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}],"machine_shape":"hm","gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}
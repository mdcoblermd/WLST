{"cells":[{"cell_type":"code","execution_count":null,"id":"332b9791","metadata":{"id":"332b9791"},"outputs":[],"source":["##install and import necessary modules\n","##this code was originally designed and run in google colab\n","##use outside of colab may require modification\n","##if using colab, you may need to restart your runtime after installing modules,\n","##depending on enviornment at time of code running.\n","##due to potential module dependencies, we will install DeepTables later\n","\n","!pip install scikit-learn==1.5.2\n","!pip install tensorflow==2.12.1\n","!pip install xgboost==2.0.2\n","!pip install shap\n","import shap\n","import sys\n","import sklearn\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import xgboost as xgb\n","import seaborn as sn\n","from google.colab import drive\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import roc_auc_score, f1_score, roc_curve, auc, precision_recall_curve, recall_score, confusion_matrix, brier_score_loss\n","from sklearn.calibration import CalibratedClassifierCV, calibration_curve\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.utils import resample\n","sn.set(style='whitegrid')\n","\n","print(\"Python version:\", sys.version)\n","print(\"scikit-learn version:\", sklearn.__version__)\n","print(\"XGBoost version:\", xgb.__version__)\n","print(\"shap version:\", shap.__version__)"]},{"cell_type":"code","execution_count":null,"id":"Ik3aE1RPVETF","metadata":{"id":"Ik3aE1RPVETF"},"outputs":[],"source":["##import your dataset\n","##mount google drive if using in colab. Replace <MOUNT_POINT> with the directory where you want to mount the drive (e.g., /content/drive).\n","drive.mount('<MOUNT_POINT>')\n","\n","# Replace <YOUR_FILE_PATH> with the actual path inside your Google Drive (e.g., My Drive/FileNameHere).\n","file_path = '<MOUNT_POINT>/<YOUR_FILE_PATH>.csv'"]},{"cell_type":"code","execution_count":null,"id":"a9e8e1be","metadata":{"id":"a9e8e1be"},"outputs":[],"source":["# Import data and specify missing values\n","data = pd.read_csv(file_path, na_values=['NA', 'N/A', 'NULL', ' ', '', '-99', '-98', '-99.0', '-99.00', '-98.0', '-98.00', 'NaN'])\n","\n","# Filter out rows where 'TRAUMATYPE' is 26, 'Other/unspecified', or 'Burn'\n","try:\n","  exclude_values = ['26', 'Other/unspecified', 'Burn']\n","  data = data[~data['TRAUMATYPE'].isin(exclude_values)]\n","except:\n","  pass"]},{"cell_type":"code","execution_count":null,"id":"522c0982","metadata":{"id":"522c0982"},"outputs":[],"source":["##check dataframe to ensure it appears as it should\n","data.head()"]},{"cell_type":"code","execution_count":null,"id":"ab349743","metadata":{"id":"ab349743"},"outputs":[],"source":["##check for missing data\n","data.isnull().sum(axis=0)"]},{"cell_type":"code","execution_count":null,"id":"e0ad2ed6","metadata":{"id":"e0ad2ed6"},"outputs":[],"source":["##create a dataframe of all complications/things not available on admission.  We can remove all of these from the X data set and pick one to be\n","#our Y dataset\n","\n","complications_df=pd.DataFrame()\n","complications_list= [\n","                    'HC_CLABSI', 'HC_DEEPSSI', 'HC_DVTHROMBOSIS', 'HC_ALCOHOLWITHDRAWAL', 'HC_CARDARREST', 'HC_CAUTI',\n","                    'HC_EMBOLISM', 'HC_EXTREMITYCS', 'HC_INTUBATION', 'HC_KIDNEY', 'HC_MI', 'HC_ORGANSPACESSI',\n","                    'HC_OSTEOMYELITIS', 'HC_RESPIRATORY', 'HC_RETURNOR', 'HC_SEPSIS', 'HC_STROKECVA', 'HC_SUPERFICIALINCISIONSSI',\n","                    'HC_PRESSUREULCER', 'HC_UNPLANNEDICU', 'HC_VAPNEUMONIA',\n","                    ##'EDDISCHARGEDISPOSITION',\n","                    'HOSPDISCHARGEDISPOSITION',\n","                    ##'EDDISCHARGEHRS',\n","                    'WITHDRAWALLST',\n","                    'VTEPROPHYLAXISTYPE',\n","                    'TOTALICULOS',\n","                    'TOTALVENTDAYS',\n","                    'VTEPROPHYLAXISHRS',\n","                    'VTEPROPHYLAXISDAYS', 'MORTALITY', 'EDDISCHARGEDAYS','FINALDISCHARGEDAYS','FINALDISCHARGEHRS', 'HMRRHGCTRLSURGDAYS',  'WITHDRAWALLSTHRS',\n","                    ##'AMERICANINDIAN', 'ASIAN', 'BLACK', 'PACIFICISLANDER', 'RACEOTHER', 'WHITE', 'RACE_NA', 'RACE_UK',\n","                    'ISS_05'\n","                    , 'AIS_FACE', 'AIS_NECK', 'AIS_HEAD', 'AIS_THORAX', 'AIS_ABDOMEN', 'AIS_SPINE', 'AIS_UPPEREX', 'AIS_LOWEREX', 'AIS_SKIN', 'AIS_OTHER'\n","                    ##, 'VTEPPXStartOver48', 'VTEPPXStartOver24', 'ICUOver48', 'ICUOver24', 'VentOver48', 'VentOver24'\n","                    , 'VTEPPXStartOver72', 'VTEPPXStartOver96', 'ICUOver72', 'ICUOver96', 'VentOver72', 'VentOver96'\n","                    , 'FacilityTotalWLST', 'factilityTotalPatients', 'FacilityWLSTRate'\n","                    , 'facilityWLSTNew', 'WLSTRateNew', 'WLSTRateNewCensored'\n","                    ]\n","for c in complications_list:\n","    complications_df[c] = data[c]\n","complications_df"]},{"cell_type":"code","execution_count":null,"id":"8650e14e","metadata":{"id":"8650e14e"},"outputs":[],"source":["##this is where we choose our outcome variable, in this case, WLST, and move it to a separate dataframe\n","Y_data = pd.DataFrame()\n","Y_data['WLST'] = data['WITHDRAWALLST']\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"75be5241","metadata":{"id":"75be5241"},"outputs":[],"source":["##clean Y_data by replacing \"Yes\" and \"No\" vcalues with 0's and 1's\n","\n","Y_data['WLST'] = Y_data['WLST'].replace({'Yes': 1, 'No': 0})\n","Y_data"]},{"cell_type":"code","execution_count":null,"id":"90944eb8","metadata":{"id":"90944eb8"},"outputs":[],"source":["##remove all unwanted variables as defined above from the input space\n","X_data = data.drop(columns=complications_list)\n","X_data.shape"]},{"cell_type":"code","source":["##need to remove any cases with missing data for our outcome variable\n","Missing_Y = Y_data.isnull().sum(axis=0)\n","Missing_Y"],"metadata":{"id":"eh992q4ujyp7"},"id":"eh992q4ujyp7","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##here we find which rows in Y have missing values\n","\n","bad_row_index_list=[]\n","for n in range(0, Y_data.shape[0]):\n","    n_missings=Y_data.iloc[n,:].isnull().sum()\n","    if n_missings>0:\n","        bad_row_index_list.append(n)\n","bad_row_index_list"],"metadata":{"id":"73arQ_SPj1Iy"},"id":"73arQ_SPj1Iy","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"b9270983","metadata":{"id":"b9270983"},"outputs":[],"source":["##now remove the bad rows in Y\n","Y_clean = Y_data.drop(bad_row_index_list, axis=0)\n","Y_clean"]},{"cell_type":"code","source":["##ensure all cases with missing values for the outcome have been dropped\n","Missing_Y_clean = Y_clean.isnull().sum(axis=0)\n","Missing_Y_clean"],"metadata":{"id":"ZWG9Tdo_gk5e"},"id":"ZWG9Tdo_gk5e","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##and remove bad rows in X\n","X_data=X_data.drop(bad_row_index_list, axis=0)"],"metadata":{"id":"M4GwsrAqj_r2"},"id":"M4GwsrAqj_r2","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"088ce1ba","metadata":{"id":"088ce1ba"},"outputs":[],"source":["##check which variables in the input space have missing variables\n","Missing = X_data.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"qaR1MA4zQX5n","metadata":{"id":"qaR1MA4zQX5n"},"outputs":[],"source":["##order variables with missing data by percentage\n","data_missing = (X_data.isnull().sum(axis=0)/X_data.shape[0]) * 100\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"FtR5sBNwQaDW","metadata":{"id":"FtR5sBNwQaDW"},"outputs":[],"source":["##display variables withOUT mising data\n","data_missing[data_missing == 0].index"]},{"cell_type":"code","execution_count":null,"id":"jCvUHe_RQbym","metadata":{"id":"jCvUHe_RQbym"},"outputs":[],"source":["#remove the good columns (no missing values) from data_missing\n","data_missing = data_missing.drop(data_missing[data_missing == 0].index)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"5qbwbipXQdnA","metadata":{"id":"5qbwbipXQdnA"},"outputs":[],"source":["#sort this in ascending order\n","data_missing = data_missing.sort_values(ascending=False)\n","data_missing"]},{"cell_type":"code","execution_count":null,"id":"pvKzlMEpQfcT","metadata":{"id":"pvKzlMEpQfcT"},"outputs":[],"source":["##prepare to drop variables with >50% missing values\n","dropCutoff=50\n","bad_column_names = data_missing[data_missing >=dropCutoff].index\n","bad_column_names"]},{"cell_type":"code","execution_count":null,"id":"uMsxdfhfQg2M","metadata":{"id":"uMsxdfhfQg2M"},"outputs":[],"source":["##actually drop bad variables\n","X_data_new=X_data.drop(columns=bad_column_names, axis=1)\n","\n","##check for which variables still have missing data (<50% missing values)\n","Missing = X_data_new.isnull().sum(axis=0)\n","Missing[Missing>0]"]},{"cell_type":"code","execution_count":null,"id":"HIBPbQv8QifC","metadata":{"id":"HIBPbQv8QifC"},"outputs":[],"source":["#check for columns with less than 50% missing that need to be cleaned\n","to_be_cleaned_column_names = data_missing[data_missing <50].index\n","to_be_cleaned_column_names"]},{"cell_type":"code","execution_count":null,"id":"b5ec271e","metadata":{"id":"b5ec271e"},"outputs":[],"source":["##perform median imputation for continuous variable and mode imputation for categorical\n","for c in to_be_cleaned_column_names:\n","    v=X_data_new[c]#get values in this column\n","    v_valid=v[~v.isnull()] # get valid values\n","    if X_data_new[c].dtype == np.dtype('O'): # non-numeric values\n","        X_data_new[c]=X_data_new[c].fillna(v.value_counts().index[0]).astype(object) # the most frequent category\n","    else: # numeric\n","        X_data_new[c]=X_data_new[c].fillna(v_valid.median()) #replace nan with median value"]},{"cell_type":"code","execution_count":null,"id":"31a5eafa","metadata":{"id":"31a5eafa"},"outputs":[],"source":["##confirm no more missing data in imput space\n","X_data_new.isnull().sum().sum()"]},{"cell_type":"code","execution_count":null,"id":"0a109d01","metadata":{"id":"0a109d01"},"outputs":[],"source":["##verify cleaned dataframe appears as intended\n","X_data_new.head()"]},{"cell_type":"code","execution_count":null,"id":"iWR41vor-_Pu","metadata":{"id":"iWR41vor-_Pu"},"outputs":[],"source":["# Rename the 'TRAUMATYPE' column to 'Penetrating' and map the values to 0 and 1\n","X_data_new['Penetrating'] = X_data_new['TRAUMATYPE'].map({'Penetrating': 1, 'Blunt': 0})\n","\n","# Drop the old 'TRAUMATYPE' column\n","X_data_new.drop(columns=['TRAUMATYPE'], inplace=True)"]},{"cell_type":"code","execution_count":null,"id":"c14955b3","metadata":{"id":"c14955b3"},"outputs":[],"source":["# Display the entire DataFrame without truncation\n","pd.set_option('display.max_columns', None)\n","\n","# Get column names and data types\n","columns_info = []\n","for column_name, dtype in zip(X_data_new.columns, X_data_new.dtypes):\n","    columns_info.append(f\"{column_name}: {dtype}\")\n","\n","formatted_columns_info = \"\\n\".join(columns_info)\n","\n","# Print column names and data types\n","print(\"Column Names and Data Types:\")\n","print(formatted_columns_info)"]},{"cell_type":"code","source":["##remove any additional variables necessary\n","## Remove the \"RACE\" and \"TRANSPORTMODE\" columns, as these are composite varibles that have already been 1 hot encoded\n","columns_to_remove = ['RACE', 'TRANSPORTMODE']\n","X_data_new = X_data_new.drop(columns=columns_to_remove, errors='ignore')"],"metadata":{"id":"aHrlRyyIkk5V"},"id":"aHrlRyyIkk5V","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"2727c717","metadata":{"id":"2727c717"},"outputs":[],"source":["##first we will convert No's and Yes's to 0's and 1's to minimize the amount of double variables (want to avoid Yes/Nos being converted to 1-hot variables)\n","##want code to be reusable between different populations of input data.  Not every population will have all of these variables\n","##Therefore, will do everything within separate try/except blocks\n","\n","try:\n","    X_data_new= X_data_new.replace({True: 1, 'Yes': 1, \"Female\": 1, False: 0, 'No': 0, \"Male\": 0})\n","except:\n","    pass\n","try:\n","    X_data_new['ETHNICITY'] = X_data_new['ETHNICITY'].replace({'Hispanic or Latino': 1, 'Not Hispanic or Latino': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSEYE'] = X_data_new['EMSGCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3,\n","                                                               'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSEYE'] = X_data_new['GCSEYE'].replace({'None': 1, 'To pressure': 2, 'To sound': 3, 'Spontaneous': 4})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSVERBAL'] = X_data_new['EMSGCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                                     'Confused': 4, 'Oriented': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['EMSGCSMOTOR'] = X_data_new['EMSGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['TBIGCSMOTOR'] = X_data_new['TBIGCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                                 'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSVERBAL'] = X_data_new['GCSVERBAL'].replace({'None': 1, 'Sounds': 2, 'Words': 3,\n","                                                               'Confused': 4, 'Orientated': 5})\n","except:\n","    pass\n","try:\n","    X_data_new['GCSMOTOR'] = X_data_new['GCSMOTOR'].replace({'None': 1, 'Extension': 2, 'Abnormal Flexion': 3,\n","                                                           'Normal Flexion': 4, 'Localising': 5, 'Obeys commands': 6})\n","except:\n","    pass\n","try:\n","    X_data_new['RESPIRATORYASSISTANCE'] = X_data_new['RESPIRATORYASSISTANCE'].replace({'Assisted Respiratory Rate': 1,\n","                                                                                   'Unassisted Respiratory Rate': 0})\n","except:\n","    pass\n","try:\n","    X_data_new['SUPPLEMENTALOXYGEN'] = X_data_new['SUPPLEMENTALOXYGEN'].replace({'Supplemental Oxygen': 1,\n","                                                                             'No Supplemental Oxygen': 0})\n","except:\n","    pass\n","\n","X_data_new.head()\n","\n","##male coded as 0\n","##female coded as 1\n","\n","##not hispanic coded as 0\n","##hispanic coded as 1"]},{"cell_type":"code","execution_count":null,"id":"3f17738d","metadata":{"id":"3f17738d"},"outputs":[],"source":["##need to convert categorical values to numerical values using one-hot encoding\n","categorical_column=[]\n","for c in X_data_new.columns:\n","    if X_data_new[c].dtype == np.dtype('O', 'category'): # non-numeric values\n","        categorical_column.append(c)\n","categorical_column"]},{"cell_type":"code","execution_count":null,"id":"2770257a","metadata":{"id":"2770257a"},"outputs":[],"source":["##check how many variables we need to one-hot encode\n","len(categorical_column)"]},{"cell_type":"code","execution_count":null,"id":"60e3b4bf","metadata":{"id":"60e3b4bf"},"outputs":[],"source":["##verify dataframe shape\n","X_data_new.shape"]},{"cell_type":"code","execution_count":null,"id":"323357b1","metadata":{"id":"323357b1"},"outputs":[],"source":["##one-hot encode variables above\n","X_clean=pd.get_dummies(X_data_new, columns=categorical_column, sparse=False)\n","X_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"c524ec1d","metadata":{"id":"c524ec1d"},"outputs":[],"source":["##verify cleaned true label dataframe shape\n","Y_clean.shape"]},{"cell_type":"code","execution_count":null,"id":"44d987e0","metadata":{"id":"44d987e0"},"outputs":[],"source":["##verify no missing data in the cleaned input space\n","X_clean.isnull().sum().sum()"]},{"cell_type":"code","source":["##drop patient ID's\n","X_clean.drop(['inc_key'], axis=1, inplace=True)"],"metadata":{"id":"Qc1yYzaQk2T8"},"id":"Qc1yYzaQk2T8","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"630434e8","metadata":{"id":"630434e8"},"outputs":[],"source":["##replace boolean values in binary variables to numeric values\n","X_clean = X_clean.replace({True: 1, False: 0})"]},{"cell_type":"code","execution_count":null,"id":"mrWz__0NRJtw","metadata":{"id":"mrWz__0NRJtw"},"outputs":[],"source":["##verify dataframe appears as intended\n","X_clean.head()"]},{"cell_type":"code","execution_count":null,"id":"AbyaxTByRK__","metadata":{"id":"AbyaxTByRK__"},"outputs":[],"source":["##split cleaned input space into training and test sets\n","X_train, X_test, Y_train, Y_test = train_test_split(X_clean, Y_clean, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"62nD6XkiRMfn","metadata":{"id":"62nD6XkiRMfn"},"outputs":[],"source":["##Before converting to Numpy arrays, we generate copies of thed data in tensor format to ensure we have access to\n","##tensor format data if needed\n","\n","X_train_tensor=X_train.copy()\n","Y_train_tensor=Y_train.copy()\n","X_test_tensor=X_test.copy()\n","Y_test_tesnor=Y_test.copy()"]},{"cell_type":"code","execution_count":null,"id":"MkSkefq4RN9D","metadata":{"id":"MkSkefq4RN9D"},"outputs":[],"source":["##convert sets to Numpy arrays:\n","X_train=X_train.values\n","Y_train=Y_train.values.reshape(-1)\n","X_test=X_test.values\n","Y_test=Y_test.values.reshape(-1)"]},{"cell_type":"code","execution_count":null,"id":"PYoFPWDoRPQW","metadata":{"id":"PYoFPWDoRPQW"},"outputs":[],"source":["##now we have X_train, Y_train, X_test, Y_test as numpy arrays\n","\n","scaler=StandardScaler()\n","#get the parameters of the transform\n","scaler.fit(X_train)\n","#normalize the features in the training set\n","X_train_s = scaler.transform(X_train)\n","#normalize the features in the test set\n","X_test_s = scaler.transform(X_test)\n","\n","##lets also scale the tensor copies we created\n","X_train_tensor_s = scaler.transform(X_train_tensor)\n","X_test_tensor_s = scaler.transform(X_test_tensor)"]},{"cell_type":"code","execution_count":null,"id":"E1DWiDLua903","metadata":{"id":"E1DWiDLua903"},"outputs":[],"source":["##further split the training set into a training and validation/calibration set\n","X_train_s_cal, X_val_s_cal, Y_train_cal, Y_val_cal = train_test_split(X_train_s, Y_train, test_size=0.2, random_state=0)"]},{"cell_type":"code","execution_count":null,"id":"vtemq7fdbjoE","metadata":{"id":"vtemq7fdbjoE"},"outputs":[],"source":["##now, initialize XGBoost model using parameters determined from gridsearchCV hyperparameter optimization\n","model_gb=xgb.XGBClassifier(random_state=0)\n","model_best_gb = xgb.XGBClassifier(random_state=0, colsample_bytree=0.6, learning_rate=0.1, max_depth=7, n_estimators=150, subsample=0.8)"]},{"cell_type":"code","execution_count":null,"id":"L9Wj3__pbnO6","metadata":{"id":"L9Wj3__pbnO6"},"outputs":[],"source":["# Get predicted probabilities for the positive class (WLST)\n","model_best_gb.fit(X_train_s_cal, Y_train_cal)\n","y_prob_gbo_mtp = model_best_gb.predict_proba(X_test_s)[:, 1]\n","\n","# Compute AUROC\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo_mtp)\n","print(f\"AUROC on the test set: {auroc_gbo}\")"]},{"cell_type":"code","source":["# Calibrate the model on the validation set\n","calibrated_model = CalibratedClassifierCV(estimator=model_best_gb, method='isotonic', cv='prefit')\n","calibrated_model.fit(X_val_s_cal, Y_val_cal)"],"metadata":{"id":"KzRmMMFYqgJF"},"id":"KzRmMMFYqgJF","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 1. Get predicted probabilities for the positive class in calibrated model (WLST)\n","# ========================================\n","y_prob_gbo = calibrated_model.predict_proba(X_test_s)[:, 1]\n","\n","# ========================================\n","# 2. Basic Metrics + Curves\n","# ========================================\n","# ------------------------- #\n","#       ROC Curve\n","# ------------------------- #\n","auroc_gbo = roc_auc_score(Y_test, y_prob_gbo)\n","print(f\"AUROC on the test set: {auroc_gbo:.3f}\")\n","\n","fpr_gbo, tpr_gbo, thresholds_roc = roc_curve(Y_test, y_prob_gbo)\n","roc_auc_gbo = auc(fpr_gbo, tpr_gbo)\n","\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_gbo, tpr_gbo, color='darkorange', lw=2,\n","         label=f'ROC curve (AUC = {roc_auc_gbo:.3f})')\n","plt.plot([0, 1], [0, 1], color='black', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()\n","\n","# ------------------------- #\n","#  Precision-Recall Curve\n","# ------------------------- #\n","precision, recall, thresholds_pr = precision_recall_curve(Y_test, y_prob_gbo)\n","pr_auc = auc(recall, precision)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(recall, precision, color='blue', label=f'PR curve (AUC={pr_auc:.3f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc='best')\n","plt.show()\n","\n","# ========================================\n","# 3. Confusion Matrix Stats @ threshold=0.5\n","# ========================================\n","custom_threshold = 0.5\n","y_pred_custom_threshold = (y_prob_gbo >= custom_threshold).astype(int)\n","\n","conf_matrix = confusion_matrix(Y_test, y_pred_custom_threshold)\n","falseNeg, falsePos, trueNeg, truePos = conf_matrix[1, 0], conf_matrix[0, 1], conf_matrix[0, 0], conf_matrix[1, 1]\n","actualPos, actualNeg = truePos + falseNeg, trueNeg + falsePos\n","predPos, predNeg = truePos + falsePos, trueNeg + falseNeg\n","\n","fpr = falsePos / actualNeg if actualNeg else 0\n","fnr = falseNeg / actualPos if actualPos else 0\n","tpr = truePos / actualPos if actualPos else 0\n","tnr = trueNeg / actualNeg if actualNeg else 0\n","accuracy = (truePos + trueNeg) / (actualPos + actualNeg) if (actualPos + actualNeg) else 0\n","ppV = truePos / predPos if predPos != 0 else 0\n","npV = trueNeg / predNeg if predNeg != 0 else 0\n","\n","brier_score = brier_score_loss(Y_test, y_prob_gbo)\n","\n","print(\"\\n=== Threshold = 0.50 ===\")\n","print(\"Confusion Matrix:\")\n","print(conf_matrix)\n","print(f\"False Positive Rate (FPR): {fpr:.3f}\")\n","print(f\"False Negative Rate (FNR): {fnr:.3f}\")\n","print(f\"True Positive Rate (TPR/Recall): {tpr:.3f}\")\n","print(f\"True Negative Rate (TNR/Specificity): {tnr:.3f}\")\n","print(f\"Accuracy: {accuracy:.3f}\")\n","print(f\"Precision (PPV): {ppV:.3f}\")\n","print(f\"Negative Predictive Value (NPV): {npV:.3f}\")\n","print(f\"Brier Score: {brier_score:.4f}\")\n","\n","# ========================================\n","# 4. Find Threshold that Maximizes F1 (0.01 increments)\n","# ========================================\n","candidate_thresholds = np.linspace(0, 1, 101)  # from 0.00 to 1.00 in 0.01 steps\n","best_f1 = -1.0\n","best_f1_threshold = 0.0\n","\n","for t in candidate_thresholds:\n","    y_pred = (y_prob_gbo >= t).astype(int)\n","    f1_val = f1_score(Y_test, y_pred)\n","    if f1_val > best_f1:\n","        best_f1 = f1_val\n","        best_f1_threshold = t\n","\n","print(f\"\\nOptimal threshold for max F1 (nearest hundredth): {best_f1_threshold:.2f}\")\n","print(f\"Max F1 Score at that threshold: {best_f1:.3f}\")\n","\n","# ========================================\n","# 5. Find Threshold for >=90% Specificity (0.01 increments)\n","# ========================================\n","tnr_90_thresh = None\n","\n","for t in candidate_thresholds:\n","    y_pred = (y_prob_gbo >= t).astype(int)\n","    cm = confusion_matrix(Y_test, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","    actual_neg = tn + fp\n","    if actual_neg == 0:\n","        continue\n","\n","    specificity = tn / actual_neg  # TNR\n","    if specificity >= 0.90:\n","        tnr_90_thresh = t\n","        break  # first threshold that achieves TNR >= 0.90\n","\n","if tnr_90_thresh is not None:\n","    print(f\"\\nThreshold achieving >=90% specificity (nearest hundredth): {tnr_90_thresh:.2f}\")\n","    # Recompute confusion matrix & TPR at that threshold\n","    y_pred_90sp = (y_prob_gbo >= tnr_90_thresh).astype(int)\n","    cm_90 = confusion_matrix(Y_test, y_pred_90sp)\n","    tn_90, fp_90, fn_90, tp_90 = cm_90.ravel()\n","    tnr_val_90 = tn_90 / (tn_90 + fp_90) if (tn_90 + fp_90) else 0\n","    tpr_val_90 = tp_90 / (tp_90 + fn_90) if (tp_90 + fn_90) else 0\n","    print(f\"Specificity (TNR) at {tnr_90_thresh:.2f}: {tnr_val_90:.3f}\")\n","    print(f\"Sensitivity (TPR) at {tnr_90_thresh:.2f}: {tpr_val_90:.3f}\")\n","else:\n","    print(\"\\nNo threshold found where TNR >= 0.90 in [0.00, 1.00].\")\n","\n","# ========================================\n","# 6. Decision Curve\n","# ========================================\n","def net_benefit(y_true, y_prob, thresholds):\n","    \"\"\"\n","    NetBenefit = (TP/N) - (FP/N)*(threshold/(1-threshold))\n","    \"\"\"\n","    N = len(y_true)\n","    NB = []\n","    for t in thresholds:\n","        y_pred = (y_prob >= t).astype(int)\n","        TP = np.sum((y_true == 1) & (y_pred == 1))\n","        FP = np.sum((y_true == 0) & (y_pred == 1))\n","        if t == 1.0:\n","            nb_t = 0\n","        else:\n","            nb_t = (TP / N) - (FP / N) * (t / (1 - t))\n","        NB.append(nb_t)\n","    return NB\n","\n","decision_thresholds = np.linspace(0.0, 1.0, 101)\n","NB_model = net_benefit(Y_test, y_prob_gbo, decision_thresholds)\n","\n","N = len(Y_test)\n","prevalence = np.mean(Y_test)  # fraction of actual positives\n","treat_all_nb = []\n","for t in decision_thresholds:\n","    if t == 1.0:\n","        treat_all_nb.append(0)\n","    else:\n","        treat_all_nb.append(prevalence - (1 - prevalence)*(t/(1-t)))\n","\n","treat_none_nb = np.zeros_like(decision_thresholds)\n","\n","plt.figure(figsize=(8, 6))\n","plt.plot(decision_thresholds, NB_model, label='Model', color='darkorange')\n","plt.plot(decision_thresholds, treat_all_nb, label='Treat All', color='red', linestyle='--')\n","plt.plot(decision_thresholds, treat_none_nb, label='Treat None', color='blue', linestyle=':')\n","plt.xlabel('Threshold Probability')\n","plt.ylabel('Net Benefit')\n","plt.title('Decision Curve Analysis')\n","plt.legend(loc='best')\n","plt.ylim([-0.3, 0.3])\n","plt.xlim([0, 1.0])\n","plt.show()\n","\n","# ========================================\n","# 7. Define a Function to Compute Metrics\n","# ========================================\n","def compute_metrics(y_true, y_prob, threshold):\n","    \"\"\"\n","    Computes performance metrics at a given threshold.\n","\n","    Parameters:\n","    - y_true: True binary labels\n","    - y_prob: Predicted probabilities for the positive class\n","    - threshold: Threshold to convert probabilities to binary predictions\n","\n","    Returns:\n","    - metrics_dict: Dictionary containing performance metrics\n","    \"\"\"\n","    y_pred = (y_prob >= threshold).astype(int)\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","\n","    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) else 0\n","    sensitivity = tp / (tp + fn) if (tp + fn) else 0  # TPR\n","    specificity = tn / (tn + fp) if (tn + fp) else 0  # TNR\n","    precision = tp / (tp + fp) if (tp + fp) else 0      # PPV\n","    npv = tn / (tn + fn) if (tn + fn) else 0            # NPV\n","    f1 = f1_score(y_true, y_pred) if (tp + fp + fn) else 0\n","\n","    metrics_dict = {\n","        'Accuracy': accuracy,\n","        'Sensitivity (TPR)': sensitivity,\n","        'Specificity (TNR)': specificity,\n","        'Precision (PPV)': precision,\n","        'Negative Predictive Value (NPV)': npv,\n","        'F1 Score': f1\n","    }\n","\n","    return metrics_dict\n","\n","# ========================================\n","# 8. Create and Print Performance Tables\n","# ========================================\n","# Initialize an empty list to store metric dictionaries\n","performance_metrics = []\n","\n","# Thresholds to report\n","thresholds_to_report = {\n","    'Max F1 Score Threshold': best_f1_threshold,\n","    '90% Specificity Threshold': tnr_90_thresh\n","}\n","\n","# Compute metrics for each threshold\n","for desc, thresh in thresholds_to_report.items():\n","    if thresh is not None:\n","        metrics = compute_metrics(Y_test, y_prob_gbo, thresh)\n","        metrics['Threshold'] = f\"{thresh:.2f}\"\n","        performance_metrics.append(metrics)\n","    else:\n","        print(f\"\\n{desc} is not available as no threshold achieved >=90% specificity.\")\n","\n","# Create a DataFrame\n","df_performance = pd.DataFrame(performance_metrics)\n","\n","# Reorder columns for better readability\n","df_performance = df_performance[['Threshold', 'Accuracy', 'Sensitivity (TPR)',\n","                                 'Specificity (TNR)', 'Precision (PPV)',\n","                                 'Negative Predictive Value (NPV)', 'F1 Score']]\n","\n","# Set descriptive index\n","df_performance.index = thresholds_to_report.keys()\n","\n","# Display the tables\n","print(\"\\n=== Performance Metrics at Selected Thresholds ===\")\n","print(df_performance.to_string(float_format=\"{:.3f}\".format))"],"metadata":{"id":"8ff_LH5Fabxj"},"id":"8ff_LH5Fabxj","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"akIGfeUxrIiR","metadata":{"id":"akIGfeUxrIiR"},"outputs":[],"source":["##generate reliability diagram with 95% confidence interval to assess calibration\n","\n","def bootstrap_calibration_curve(y_true, y_prob, n_bins=10, n_boot=1000, random_state=None):\n","    \"\"\"\n","    1) Compute the original bin-based calibration curve.\n","    2) Bootstrap the dataset n_boot times, each time recalculating the bin-based\n","       fraction of positives (prob_true) and storing it.\n","    3) Return the original curve + 95% CI per bin (based on 2.5 and 97.5 percentiles).\n","    \"\"\"\n","    # -----------------------------\n","    # Original calibration curve\n","    # -----------------------------\n","    # prob_true_orig, prob_pred_orig = calibration_curve(...) does binning internally.\n","    # But we want to fix n_bins and ensure consistent binning across bootstraps.\n","    # We'll do a manual binning approach here to keep consistent bin boundaries.\n","\n","    # Define bin edges (equally spaced from 0 to 1)\n","    bin_edges = np.linspace(0, 1, n_bins + 1)\n","    # Digitize predicted probabilities\n","    bin_indices = np.digitize(y_prob, bin_edges) - 1\n","    bin_indices[bin_indices == n_bins] = n_bins - 1  # cap any == n_bins to last bin\n","\n","    # Prepare arrays to hold the original bin stats\n","    prob_pred_orig = np.zeros(n_bins)\n","    prob_true_orig = np.zeros(n_bins)\n","    counts_in_bin = np.zeros(n_bins, dtype=int)\n","\n","    # Fill in the stats for each bin\n","    for i in range(n_bins):\n","        mask = (bin_indices == i)\n","        counts_in_bin[i] = np.sum(mask)\n","        if counts_in_bin[i] > 0:\n","            prob_pred_orig[i] = np.mean(y_prob[mask])   # mean predicted prob in this bin\n","            prob_true_orig[i] = np.mean(y_true[mask])   # fraction of positives (actual)\n","        else:\n","            # If bin is empty, set to NaN\n","            prob_pred_orig[i] = np.nan\n","            prob_true_orig[i] = np.nan\n","\n","    # Remove empty bins (NaN) from the original arrays\n","    valid_mask = ~np.isnan(prob_pred_orig)\n","    prob_pred_orig = prob_pred_orig[valid_mask]\n","    prob_true_orig = prob_true_orig[valid_mask]\n","\n","    # -----------------------------\n","    # Bootstrap to get CIs\n","    # -----------------------------\n","    rng = np.random.RandomState(random_state) if random_state else np.random\n","\n","    # We'll store the fraction of positives (prob_true) for each bin in each bootstrap\n","    # but only for the bins that were valid in the original data\n","    boot_prob_true = np.zeros((n_boot, sum(valid_mask)))\n","\n","    n_data = len(y_true)\n","    data_idx = np.arange(n_data)\n","\n","    for b in range(n_boot):\n","        # Sample with replacement\n","        sample_indices = rng.randint(0, n_data, size=n_data)\n","        y_true_b = y_true[sample_indices]\n","        y_prob_b = y_prob[sample_indices]\n","\n","        # Repeat the binning steps\n","        bin_indices_b = np.digitize(y_prob_b, bin_edges) - 1\n","        bin_indices_b[bin_indices_b == n_bins] = n_bins - 1\n","\n","        prob_true_b = np.zeros(n_bins)\n","        for i in range(n_bins):\n","            mask_b = (bin_indices_b == i)\n","            if np.sum(mask_b) > 0:\n","                prob_true_b[i] = np.mean(y_true_b[mask_b])\n","            else:\n","                prob_true_b[i] = np.nan\n","\n","        # filter to only valid bins\n","        prob_true_b = prob_true_b[valid_mask]\n","        boot_prob_true[b, :] = prob_true_b\n","\n","    # Compute 2.5th and 97.5th percentile per bin (column-wise)\n","    lower_ci = np.nanpercentile(boot_prob_true, 2.5, axis=0)\n","    upper_ci = np.nanpercentile(boot_prob_true, 97.5, axis=0)\n","\n","    return prob_pred_orig, prob_true_orig, lower_ci, upper_ci\n","\n","# -----------------------------------------------------------\n","# Now actually generate the figure\n","# -----------------------------------------------------------\n","\n","# 1) Compute the calibration data + bootstrap CIs\n","prob_pred_orig, prob_true_orig, lower_ci, upper_ci = bootstrap_calibration_curve(\n","    Y_test, y_prob_gbo, n_bins=10, n_boot=1000, random_state=42\n",")\n","\n","# 2) Brier score (overall calibration measure)\n","brier_score = brier_score_loss(Y_test, y_prob_gbo)\n","print(f\"Brier Score: {brier_score:.4f}\")\n","\n","# 3) Plot\n","plt.figure(figsize=(8,6))\n","# Perfect calibration line\n","plt.plot([0,1],[0,1], 'k--', label='Perfect Calibration')\n","\n","# Original calibration line\n","# Sort by prob_pred_orig so the line doesn't jump around\n","sort_idx = np.argsort(prob_pred_orig)\n","x_sorted = prob_pred_orig[sort_idx]\n","y_sorted = prob_true_orig[sort_idx]\n","lower_sorted = lower_ci[sort_idx]\n","upper_sorted = upper_ci[sort_idx]\n","\n","plt.plot(x_sorted, y_sorted, marker='o', color='b', label='Calibration Curve')\n","\n","# 4) Fill the 95% CI region\n","plt.fill_between(x_sorted, lower_sorted, upper_sorted, color='b', alpha=0.2,\n","                 label='95% CI')\n","\n","plt.xlabel('Mean Predicted Probability')\n","plt.ylabel('Fraction of Positives')\n","plt.title('Reliability Diagram with 95% Confidence Interval (CI)')\n","plt.legend(loc='best')\n","plt.xlim([0,1])\n","plt.ylim([0,1])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"mwuiRojPF6ah","metadata":{"id":"mwuiRojPF6ah"},"outputs":[],"source":["##now, comput 95% confidence interval based on 1000-sample bootstrapping\n","\n","# Initialize your classifier (replace with your actual classifier)\n","classifier = model_best_gb\n","\n","# Compute the AUROC on the original test set\n","original_auroc = auroc_gbo\n","\n","\n","\n","# Number of bootstrap samples\n","n_bootstrap_samples = 1000\n","\n","# Initialize an array to store bootstrapped AUROC values\n","bootstrap_aurocs = np.zeros(n_bootstrap_samples)\n","\n","# Perform bootstrapping\n","for i in range(n_bootstrap_samples):\n","    # Generate a bootstrap sample\n","    bootstrap_indices = np.random.choice(len(Y_test_tesnor), len(Y_test_tesnor), replace=True)\n","    y_bootstrap = y_prob_gbo_mtp[bootstrap_indices]\n","    y_true_bootstrap = Y_test_tesnor.iloc[bootstrap_indices]\n","\n","    # Compute AUROC on the bootstrap sample\n","    bootstrap_aurocs[i] = roc_auc_score(y_true_bootstrap, y_bootstrap)\n","\n","# Calculate the confidence interval\n","confidence_interval = np.percentile(bootstrap_aurocs, [2.5, 97.5])\n","\n","print(\"Original AUROC:\", original_auroc)\n","print(\"95% Confidence Interval:\", confidence_interval)"]},{"cell_type":"code","execution_count":null,"id":"PfsLE1nlf9yO","metadata":{"id":"PfsLE1nlf9yO"},"outputs":[],"source":["##now, comput 95% confidence interval of the AUPRC based on 1000-sample bootstrapping\n","\n","\n","# Number of bootstrap iterations\n","n_iterations = 1000\n","# List to store bootstrapped AUPRC values\n","auprc_scores = []\n","\n","# Original precision-recall calculation for actual data\n","precision, recall, thresholds = precision_recall_curve(Y_test, y_prob_gbo_mtp)\n","pr_auc = auc(recall, precision)\n","\n","# Bootstrapping process to calculate 95% CI\n","for i in range(n_iterations):\n","    # Resample Y_test and y_prob_gbo_mtp with replacement\n","    Y_test_resampled, y_prob_resampled = resample(Y_test, y_prob_gbo_mtp)\n","\n","    # Calculate precision and recall for the bootstrap sample\n","    precision_resampled, recall_resampled, _ = precision_recall_curve(Y_test_resampled, y_prob_resampled)\n","\n","    # Calculate AUPRC for the resampled data\n","    pr_auc_resampled = auc(recall_resampled, precision_resampled)\n","\n","    # Store the AUPRC score\n","    auprc_scores.append(pr_auc_resampled)\n","\n","# Calculate the 95% confidence interval (2.5th and 97.5th percentiles)\n","lower_bound = np.percentile(auprc_scores, 2.5)\n","upper_bound = np.percentile(auprc_scores, 97.5)\n","\n","# Print the original AUPRC and 95% CI\n","print(f'AUPRC: {pr_auc:.3f}')\n","print(f'95% CI for AUPRC: [{lower_bound:.3f}, {upper_bound:.3f}]')"]},{"cell_type":"code","execution_count":null,"id":"_GqRJDC_rQ8B","metadata":{"id":"_GqRJDC_rQ8B"},"outputs":[],"source":["##now, use Shapley Additive Explanations for better assessment and visualization of feature imprtance\n","\n","your_dataframe = X_train_tensor  # will use this to get column labels, so need the tensor\n","model=model_best_gb\n","\n","# Calculate SHAP values for X_test\n","explainer = shap.TreeExplainer(model)\n","shap_values_test = explainer.shap_values(X_train_s_cal)\n","\n","# Calculate mean absolute SHAP values\n","mean_abs_shap = np.abs(shap_values_test).mean(axis=0)\n","\n","# Sort feature indices based on mean absolute SHAP values\n","sorted_indices = np.argsort(mean_abs_shap)\n","\n","# Identify top 20 most important features\n","top_5_percent_indices = sorted_indices[-20:]\n","\n","# Extract top 20 SHAP values and features\n","top_5_percent_shap_values = shap_values_test[:, top_5_percent_indices]\n","top_5_percent_feature_names = your_dataframe.columns[top_5_percent_indices]\n","\n","# Create horizontal bar chart for top 20 most important features\n","fig1, ax1 = plt.subplots(figsize=(12, 6))\n","bars = ax1.barh(top_5_percent_feature_names, mean_abs_shap[top_5_percent_indices], color='lightblue')\n","ax1.set_xlabel('Mean Absolute SHAP Value')\n","ax1.set_title('Top 5% Most Important Features - Mean Absolute SHAP Values')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"iN4ZMEiEsZIh","metadata":{"id":"iN4ZMEiEsZIh"},"outputs":[],"source":["##now to create a SHAP beeswarm plot\n","model=model_best_gb\n","\n","# Create a SHAP explainer object\n","explainer = shap.Explainer(model)\n","\n","# Calculate SHAP values for the test data\n","shap_values = explainer.shap_values(X_train_s_cal)\n","\n","# Generate a summary plot\n","fig, ax = plt.subplots()\n","shap.summary_plot(shap_values, X_train_s_cal, feature_names=X_train_tensor.columns, max_display=20)"]},{"cell_type":"code","execution_count":null,"id":"Pga633u7dr9m","metadata":{"id":"Pga633u7dr9m"},"outputs":[],"source":["##now, initialize penalized regression model with parameters based on GridsearchCV optimization and fit\n","model_best_lr=LogisticRegression(random_state=0, C=0.01, max_iter=300, penalty='l1', solver='liblinear')\n","model_best_lr.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","execution_count":null,"id":"sz65_S6Hxgjp","metadata":{"id":"sz65_S6Hxgjp"},"outputs":[],"source":["##test model\n","y_pred_prob_lro = model_best_lr.predict_proba(X_test_s)[:, 1]\n","auroc_lro = roc_auc_score(Y_test, y_pred_prob_lro)\n","print(f'AUROC: {auroc_lro}')"]},{"cell_type":"code","execution_count":null,"id":"jG3HRdYEyfk8","metadata":{"id":"jG3HRdYEyfk8"},"outputs":[],"source":["##now, initialize random forest model with parameters based on GridsearchCV optimization and fit\n","model_best_rf=RandomForestClassifier(max_depth=30, max_features = 'sqrt', min_samples_split=5,\n","                                     min_samples_leaf=2, n_estimators=400, random_state=0)\n","model_best_rf.fit(X_train_s_cal, Y_train_cal)"]},{"cell_type":"code","source":["##test model\n","y_pred_prob_rfo = model_best_rf.predict_proba(X_test_s)[:, 1]\n","auroc_rfo = roc_auc_score(Y_test, y_pred_prob_rfo)\n","print(f'AUROC: {auroc_rfo}')"],"metadata":{"id":"u3hhyVmckPiY"},"id":"u3hhyVmckPiY","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##now, initialize KNN model with parameters based on GridsearchCV optimization and fit\n","model_best_knn=KNeighborsClassifier(n_neighbors=807)\n","model_best_knn.fit(X_train_s_cal, Y_train_cal)"],"metadata":{"id":"RpYOnb_-QT7Q"},"id":"RpYOnb_-QT7Q","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##test model\n","y_pred_prob_knno = model_best_knn.predict_proba(X_test_s)[:, 1]\n","auroc_knno = roc_auc_score(Y_test, y_pred_prob_knno)\n","print(f'AUROC: {auroc_knno}')"],"metadata":{"id":"0__yh666jWbz"},"id":"0__yh666jWbz","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##copy existing dataframes to use in neural networks\n","\n","X_clean_nn_test=X_test_s.copy()\n","Y_clean_nn_test=Y_test.copy()\n","\n","X_clean_nn_train=X_train_s_cal.copy()\n","Y_clean_nn_train=Y_train_cal.copy()\n","\n","X_clean_nn_cal=X_val_s_cal.copy()\n","Y_clean_nn_cal=Y_val_cal.copy()"],"metadata":{"id":"hRdxn8bWcaEs"},"id":"hRdxn8bWcaEs","execution_count":null,"outputs":[]},{"cell_type":"code","source":["##ensure data is in pandas dataframe\n","X_train_df = pd.DataFrame(X_clean_nn_train)\n","Y_train_s = pd.Series(Y_clean_nn_train)\n","\n","X_val_df = pd.DataFrame(X_clean_nn_cal)\n","Y_val_s = pd.Series(Y_clean_nn_cal)\n","\n","X_test_df = pd.DataFrame(X_clean_nn_test)\n","Y_test_s = pd.Series(Y_clean_nn_test)"],"metadata":{"id":"4aCtLN1wjbej"},"id":"4aCtLN1wjbej","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install deeptables\n","##revert to sklearn 1.5 to resolve dependency issues\n","!pip install scikit-learn==1.5\n","import deeptables\n","print(\"dt version:\", deeptables.__version__)\n","from deeptables.models.deeptable import DeepTable, ModelConfig\n","from deeptables.models.deepnets import DCN"],"metadata":{"id":"uIL1PftaqDBy"},"id":"uIL1PftaqDBy","execution_count":null,"outputs":[]},{"cell_type":"code","source":["#initialize neural network model and fit\n","# `auto_discrete` is used to decide wether to discretize continous varibles automatically.\n","conf = ModelConfig(\n","    nets=DCN,\n","    metrics=['AUC', 'accuracy'],\n","    auto_discrete=True\n",")\n","dt = DeepTable(config=conf)\n","model, history = dt.fit( X_train_df, Y_train_s, epochs=100, validation_data=(X_val_df, Y_val_s))\n","score = dt.evaluate(X_test_df, Y_test_s)\n","preds = dt.predict(X_test_df)"],"metadata":{"id":"81uGxP9ycbfK"},"id":"81uGxP9ycbfK","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculate ROC curve\n","y_pred_prob_ANN = dt.predict_proba(X_clean_nn_test)[:, 1]\n","fpr_ANN, tpr_ANN, thresholds = roc_curve(Y_clean_nn_test, y_pred_prob_ANN)\n","\n","# Calculate the Area Under the ROC Curve (AUC)\n","roc_auc_ANN = auc(fpr_ANN, tpr_ANN)\n","\n","# Plot ROC curve\n","plt.figure(figsize=(8, 8))\n","plt.plot(fpr_ANN, tpr_ANN, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc_ANN:.3f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('True Positive Rate')\n","plt.title('Receiver Operating Characteristic (ROC) Curve')\n","plt.legend(loc='lower right')\n","plt.show()"],"metadata":{"id":"w00K6c-1cd7n"},"id":"w00K6c-1cd7n","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1DFrM2kQIaxxoreKKfMJY8bZ75pnIBOBL","timestamp":1734555115141},{"file_id":"1XL3SGHIBPO06uNNL8pc4jbgyODiCHYmT","timestamp":1734454404403},{"file_id":"1DYwnmjwYAdIuKZSkwpMlcAF8yUXEA0x7","timestamp":1728001604398}],"machine_shape":"hm","gpuType":"V28"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":5}